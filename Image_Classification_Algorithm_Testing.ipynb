{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import struct\n",
    "import time\n",
    "\n",
    "# from sklearn import metrics\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.preprocessing import Binarizer, PowerTransformer, Normalizer\n",
    "# from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "# from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from xgboost import XGBRegressor\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3x28x28 matrix of numbers images\n",
    "with open('train-images.idx3-ubyte','rb') as f:\n",
    "    magic, size  = struct.unpack(\">II\", f.read(8))\n",
    "    nrows, ncols = struct.unpack(\">II\", f.read(8))\n",
    "    data_images  = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\n",
    "    data_images  = data_images.reshape((size, nrows, ncols))\n",
    "    data_images  = data_images[8:]\n",
    "\n",
    "#labels from the images previously loaded\n",
    "with open('train-labels.idx1-ubyte','rb') as f:\n",
    "    magic, size  = struct.unpack(\">II\", f.read(8))\n",
    "    nrows, ncols = struct.unpack(\">II\", f.read(8))\n",
    "    data_labels  = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>')).astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL and batching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating test and train sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(data_images.reshape((59992,28*28)),data_labels,\n",
    "                                                    train_size=0.005, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the trained models\n",
    "lr_model = pickle.load(open('LogisticRegression.sav', 'rb'))\n",
    "\n",
    "dt_model = pickle.load(open('DecisionTree.sav', 'rb'))\n",
    "\n",
    "rf_model = pickle.load(open('RandomForest.sav', 'rb'))\n",
    "\n",
    "xgrb_model = pickle.load(open('XGRB.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the loaded models to predict y_test and y_train\n",
    "\n",
    "# Logistic Regression\n",
    "y_pred_lr    = lr_model.predict(X_test)\n",
    "\n",
    "y_train_lr   = lr_model.predict(X_train)\n",
    "\n",
    "# Decision Tree\n",
    "y_pred_dt    = dt_model.predict(X_test)\n",
    "\n",
    "y_train_dt   = dt_model.predict(X_train)\n",
    "\n",
    "# Random Forest\n",
    "y_pred_rf    = rf_model.predict(X_test)\n",
    "\n",
    "y_train_rf   = rf_model.predict(X_train)\n",
    "\n",
    "# XG Boost\n",
    "y_pred_xgrb  = xgrb_model.predict(X_test)\n",
    "\n",
    "y_train_xgrb = xgrb_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Score and Result Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the accuracy of each model\n",
    "accuracy_lr            = round(metrics.accuracy_score(y_test , y_pred_lr),2)\n",
    "accuracy_lr_train      = round(metrics.accuracy_score(y_train, y_train_lr),2)\n",
    "\n",
    "accuracy_dt            = round(metrics.accuracy_score(y_test , y_pred_dt),2)\n",
    "accuracy_dt_train      = round(metrics.accuracy_score(y_train, y_train_dt),2)\n",
    "\n",
    "accuracy_fr            = round(metrics.accuracy_score(y_test , y_pred_rf),2)\n",
    "accuracy_fr_train      = round(metrics.accuracy_score(y_train, y_train_rf),2)\n",
    "\n",
    "accuracy_xgbr          = round(metrics.accuracy_score(y_test.astype(np.float64) , y_pred_xgrb),2)\n",
    "accuracy_xgbr_train    = round(metrics.accuracy_score(y_train.astype(np.float64), y_train_xgrb),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataframe with the accuracy of each model with train and test data\n",
    "model_accuracy = pd.DataFrame({\"Model\"   : [\"LR\", \"DT\", \"RF\", \"XGRB\"],\n",
    "                               \"Test_AC\" : [accuracy_lr, accuracy_dt, accuracy_fr, accuracy_xgbr],\n",
    "                               \"Train_AC\": [accuracy_lr_train, accuracy_dt_train, accuracy_fr_train, accuracy_xgbr_train]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the F1 of each model\n",
    "f1_lr   = round(metrics.f1_score(y_test , y_pred_lr, average='weighted'),2)\n",
    "\n",
    "f1_dt   = round(metrics.f1_score(y_test , y_pred_dt, average='weighted'),2)\n",
    "\n",
    "f1_fr   = round(metrics.f1_score(y_test , y_pred_rf, average='weighted'),2)\n",
    "\n",
    "f1_xgbr = round(metrics.f1_score(y_test.astype(np.float64) , y_pred_xgrb, average='weighted'),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataframe with the accuracy of each model with train and test data\n",
    "model_f1 = pd.DataFrame({\"Model\"   : [\"LR\", \"DT\", \"RF\", \"XGRB\"],\n",
    "                         \"Test_F1\" : [f1_lr, f1_dt, f1_fr, f1_xgbr]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a dataframe for each prediction with the original and pred values\n",
    "\n",
    "# Logistic Regression\n",
    "pred_lr = pd.DataFrame({'real':y_test.astype(int), 'pred':y_pred_lr.astype(int)})\n",
    "\n",
    "pred_lr.loc[pred_lr['pred'] != pred_lr['real'], 'pred_false'] = 1\n",
    "\n",
    "# Decision Tree\n",
    "pred_dt = pd.DataFrame({'real':y_test.astype(int), 'pred':y_pred_dt.astype(int)})\n",
    "\n",
    "pred_dt.loc[pred_dt['pred'] != pred_dt['real'], 'pred_false'] = 1\n",
    "\n",
    "# Random Forest\n",
    "pred_rf = pd.DataFrame({'real':y_test.astype(int), 'pred':y_pred_rf.astype(int)})\n",
    "\n",
    "pred_rf.loc[pred_rf['pred'] != pred_rf['real'], 'pred_false'] = 1\n",
    "\n",
    "# XG Boost\n",
    "pred_xgrb = pd.DataFrame({'real':y_test.astype(int), 'pred':y_pred_xgrb.astype(int)})\n",
    "\n",
    "pred_xgrb.loc[pred_xgrb['pred'] != pred_xgrb['real'], 'pred_false'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a data frame for each model with the count misclassified samples per number\n",
    "\n",
    "# Logistic Regression\n",
    "pred_lr_ms   = pred_lr.drop(columns=['pred']).groupby(['real']).sum()\n",
    "\n",
    "# Decision Tree\n",
    "pred_dt_ms   = pred_dt.drop(columns=['pred']).groupby(['real']).sum()\n",
    "\n",
    "# Random Forest\n",
    "pred_rf_ms   = pred_rf.drop(columns=['pred']).groupby(['real']).sum()\n",
    "\n",
    "# XG Boost\n",
    "pred_xgrb_ms = pred_xgrb.drop(columns=['pred']).groupby(['real']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a clasification report for each model\n",
    "\n",
    "# Logistic Regression\n",
    "pred_lr_cr   = metrics.classification_report(y_test , y_pred_lr)\n",
    "\n",
    "# Decision Tree\n",
    "pred_dt_cr   = metrics.classification_report(y_test , y_pred_dt)\n",
    "\n",
    "# Random Forest\n",
    "pred_rf_cr   = metrics.classification_report(y_test , y_pred_rf)\n",
    "\n",
    "# XG Boost\n",
    "pred_xgrb_cr = metrics.classification_report(y_test.astype(np.float64) , y_pred_xgrb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n         0.0       0.84      0.92      0.88      5891\\n         1.0       0.87      0.87      0.87      6715\\n         2.0       0.79      0.70      0.74      5926\\n         3.0       0.75      0.81      0.78      6095\\n         4.0       0.70      0.84      0.77      5803\\n         5.0       0.79      0.65      0.71      5395\\n         6.0       0.91      0.75      0.82      5899\\n         7.0       0.86      0.83      0.85      6231\\n         8.0       0.68      0.78      0.73      5815\\n         9.0       0.76      0.74      0.75      5923\\n\\n    accuracy                           0.79     59693\\n   macro avg       0.80      0.79      0.79     59693\\nweighted avg       0.80      0.79      0.79     59693\\n'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_xgrb_cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
