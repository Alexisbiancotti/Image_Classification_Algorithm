{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import struct\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3x28x28 matrix of numbers images\n",
    "with open('train-images.idx3-ubyte','rb') as f:\n",
    "    magic, size  = struct.unpack(\">II\", f.read(8))\n",
    "    nrows, ncols = struct.unpack(\">II\", f.read(8))\n",
    "    data_images  = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\n",
    "    data_images  = data_images.reshape((size, nrows, ncols))\n",
    "    data_images  = data_images[8:]\n",
    "\n",
    "#labels from the images previously loaded\n",
    "with open('train-labels.idx1-ubyte','rb') as f:\n",
    "    magic, size  = struct.unpack(\">II\", f.read(8))\n",
    "    nrows, ncols = struct.unpack(\">II\", f.read(8))\n",
    "    data_labels  = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>')).astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL and batching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating test and train sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(data_images.reshape((59992,28*28)),data_labels,\n",
    "                                                    train_size=0.92, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the trained models\n",
    "lr_model = pickle.load(open('LogisticRegression.sav', 'rb'))\n",
    "\n",
    "dt_model = pickle.load(open('DecisionTree.sav', 'rb'))\n",
    "\n",
    "rf_model = pickle.load(open('RandomForest.sav', 'rb'))\n",
    "\n",
    "xgrb_model = pickle.load(open('XGRB.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the loaded models to predict y_test and y_train\n",
    "\n",
    "# Logistic Regression\n",
    "y_pred_lr    = lr_model.predict(X_test)\n",
    "\n",
    "y_train_lr   = lr_model.predict(X_train)\n",
    "\n",
    "# Decision Tree\n",
    "y_pred_dt    = dt_model.predict(X_test)\n",
    "\n",
    "y_train_dt   = dt_model.predict(X_train)\n",
    "\n",
    "# Random Forest\n",
    "y_pred_rf    = rf_model.predict(X_test)\n",
    "\n",
    "y_train_rf   = rf_model.predict(X_train)\n",
    "\n",
    "# XG Boost\n",
    "y_pred_xgrb  = xgrb_model.predict(X_test)\n",
    "\n",
    "y_train_xgrb = xgrb_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Score and Result Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the accuracy of each model\n",
    "accuracy_lr            = round(metrics.accuracy_score(y_test , y_pred_lr),2)\n",
    "accuracy_lr_train      = round(metrics.accuracy_score(y_train, y_train_lr),2)\n",
    "\n",
    "accuracy_dt            = round(metrics.accuracy_score(y_test , y_pred_dt),2)\n",
    "accuracy_dt_train      = round(metrics.accuracy_score(y_train, y_train_dt),2)\n",
    "\n",
    "accuracy_fr            = round(metrics.accuracy_score(y_test , y_pred_rf),2)\n",
    "accuracy_fr_train      = round(metrics.accuracy_score(y_train, y_train_rf),2)\n",
    "\n",
    "accuracy_xgbr          = round(metrics.accuracy_score(y_test.astype(np.float64) , y_pred_xgrb),2)\n",
    "accuracy_xgbr_train    = round(metrics.accuracy_score(y_train.astype(np.float64), y_train_xgrb),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataframe with the accuracy of each model with train and test data\n",
    "model_accuracy = pd.DataFrame({\"Model\"   : [\"LR\", \"DT\", \"RF\", \"XGRB\"],\n",
    "                               \"Test_AC\" : [accuracy_lr, accuracy_dt, accuracy_fr, accuracy_xgbr],\n",
    "                               \"Train_AC\": [accuracy_lr_train, accuracy_dt_train, accuracy_fr_train, accuracy_xgbr_train]})\n",
    "\n",
    "model_accuracy.set_index('Model',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the F1 of each model\n",
    "f1_lr   = round(metrics.f1_score(y_test , y_pred_lr, average='weighted'),2)\n",
    "\n",
    "f1_dt   = round(metrics.f1_score(y_test , y_pred_dt, average='weighted'),2)\n",
    "\n",
    "f1_fr   = round(metrics.f1_score(y_test , y_pred_rf, average='weighted'),2)\n",
    "\n",
    "f1_xgbr = round(metrics.f1_score(y_test.astype(np.float64) , y_pred_xgrb, average='weighted'),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataframe with the accuracy of each model with train and test data\n",
    "model_f1 = pd.DataFrame({\"Model\"   : [\"LR\", \"DT\", \"RF\", \"XGRB\"],\n",
    "                         \"Test_F1\" : [f1_lr, f1_dt, f1_fr, f1_xgbr]})\n",
    "\n",
    "model_f1.set_index('Model',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a dataframe for each prediction with the original and pred values\n",
    "\n",
    "# Logistic Regression\n",
    "pred_lr = pd.DataFrame({'real':y_test.astype(int), 'pred':y_pred_lr.astype(int)})\n",
    "\n",
    "pred_lr.loc[pred_lr['pred'] != pred_lr['real'], 'pred_false'] = 1\n",
    "\n",
    "# Decision Tree\n",
    "pred_dt = pd.DataFrame({'real':y_test.astype(int), 'pred':y_pred_dt.astype(int)})\n",
    "\n",
    "pred_dt.loc[pred_dt['pred'] != pred_dt['real'], 'pred_false'] = 1\n",
    "\n",
    "# Random Forest\n",
    "pred_rf = pd.DataFrame({'real':y_test.astype(int), 'pred':y_pred_rf.astype(int)})\n",
    "\n",
    "pred_rf.loc[pred_rf['pred'] != pred_rf['real'], 'pred_false'] = 1\n",
    "\n",
    "# XG Boost\n",
    "pred_xgrb = pd.DataFrame({'real':y_test.astype(int), 'pred':y_pred_xgrb.astype(int)})\n",
    "\n",
    "pred_xgrb.loc[pred_xgrb['pred'] != pred_xgrb['real'], 'pred_false'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a data frame for each model with the count misclassified samples per number\n",
    "\n",
    "# Logistic Regression\n",
    "pred_lr_ms   = pred_lr.drop(columns=['pred']).groupby(['real']).sum()\n",
    "\n",
    "# Decision Tree\n",
    "pred_dt_ms   = pred_dt.drop(columns=['pred']).groupby(['real']).sum()\n",
    "\n",
    "# Random Forest\n",
    "pred_rf_ms   = pred_rf.drop(columns=['pred']).groupby(['real']).sum()\n",
    "\n",
    "# XG Boost\n",
    "pred_xgrb_ms = pred_xgrb.drop(columns=['pred']).groupby(['real']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a clasification report for each model\n",
    "\n",
    "# Logistic Regression\n",
    "pred_lr_cr   = metrics.classification_report(y_test , y_pred_lr)\n",
    "\n",
    "# Decision Tree\n",
    "pred_dt_cr   = metrics.classification_report(y_test , y_pred_dt)\n",
    "\n",
    "# Random Forest\n",
    "pred_rf_cr   = metrics.classification_report(y_test , y_pred_rf)\n",
    "\n",
    "# XG Boost\n",
    "pred_xgrb_cr = metrics.classification_report(y_test.astype(np.float64) , y_pred_xgrb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# graphing the accuracy for each model\n",
    "model_accuracy.plot(kind='barh',figsize=(10, 10), fontsize=12)\n",
    "plt.title('Accuracy in Test vs Train', fontsize=17)\n",
    "plt.ylabel('', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test_AC</th>\n",
       "      <th>Train_AC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DT</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>0.97</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGRB</th>\n",
       "      <td>0.97</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Test_AC  Train_AC\n",
       "Model                   \n",
       "LR        0.92      0.94\n",
       "DT        0.83      0.87\n",
       "RF        0.97      1.00\n",
       "XGRB      0.97      1.00"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this quick analysis we can see that the models doenst have an evident overfiting, because the diference beetween the test and train data isnt considerable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# graphing the missclasified samples for each model\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "# Logistic Regression\n",
    "plt.subplot(221)\n",
    "\n",
    "plt.bar(height=pred_lr_ms['pred_false'], x=pred_lr_ms.index, color='red')\n",
    "plt.ylabel('Misclassified Samples', fontsize=16)\n",
    "pred_lr_ms_sum = pred_lr_ms['pred_false'].sum().astype(int)\n",
    "plt.title(f'Logistic Regression: {pred_lr_ms_sum}', fontsize=17)\n",
    "\n",
    "# Decision Tree\n",
    "plt.subplot(222)\n",
    "\n",
    "plt.bar(height = pred_dt_ms['pred_false'], x=pred_dt_ms.index, color='red')\n",
    "pred_dt_ms_sum = pred_dt_ms['pred_false'].sum().astype(int)\n",
    "plt.title(f'Decision Tree: {pred_dt_ms_sum}', fontsize=17)\n",
    "\n",
    "# Random Forest\n",
    "plt.subplot(223)\n",
    "\n",
    "plt.bar(height=pred_rf_ms['pred_false'], x=pred_rf_ms.index, color='red')\n",
    "plt.xlabel('Real Values', fontsize=16)\n",
    "plt.ylabel('Misclassified Samples', fontsize=16)\n",
    "pred_rf_ms_sum = pred_rf_ms['pred_false'].sum().astype(int)\n",
    "plt.title(f'Random Forest: {pred_rf_ms_sum}', fontsize=17)\n",
    "\n",
    "# XG Boost\n",
    "plt.subplot(224)\n",
    "\n",
    "plt.bar(height=pred_xgrb_ms['pred_false'], x=pred_xgrb_ms.index, color='red')\n",
    "plt.xlabel('Real Values', fontsize=16)\n",
    "pred_xgrb_ms_sum = pred_xgrb_ms['pred_false'].sum().astype(int)\n",
    "plt.title(f'XG Boost: {pred_xgrb_ms_sum}', fontsize=17)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasification Report for each Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96       415\n",
      "           1       0.94      0.98      0.96       523\n",
      "           2       0.91      0.88      0.90       516\n",
      "           3       0.90      0.88      0.89       494\n",
      "           4       0.92      0.94      0.93       444\n",
      "           5       0.87      0.88      0.88       430\n",
      "           6       0.94      0.97      0.95       494\n",
      "           7       0.94      0.91      0.93       538\n",
      "           8       0.89      0.88      0.88       474\n",
      "           9       0.91      0.90      0.90       472\n",
      "\n",
      "    accuracy                           0.92      4800\n",
      "   macro avg       0.92      0.92      0.92      4800\n",
      "weighted avg       0.92      0.92      0.92      4800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pred_lr_cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.93      0.91       415\n",
      "           1       0.90      0.97      0.93       523\n",
      "           2       0.83      0.82      0.83       516\n",
      "           3       0.79      0.77      0.78       494\n",
      "           4       0.80      0.80      0.80       444\n",
      "           5       0.76      0.78      0.77       430\n",
      "           6       0.89      0.86      0.88       494\n",
      "           7       0.90      0.87      0.89       538\n",
      "           8       0.81      0.73      0.77       474\n",
      "           9       0.74      0.82      0.78       472\n",
      "\n",
      "    accuracy                           0.83      4800\n",
      "   macro avg       0.83      0.83      0.83      4800\n",
      "weighted avg       0.83      0.83      0.83      4800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pred_dt_cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       415\n",
      "           1       0.98      0.99      0.99       523\n",
      "           2       0.96      0.97      0.97       516\n",
      "           3       0.96      0.94      0.95       494\n",
      "           4       0.96      0.98      0.97       444\n",
      "           5       0.97      0.97      0.97       430\n",
      "           6       0.99      0.99      0.99       494\n",
      "           7       0.98      0.97      0.97       538\n",
      "           8       0.96      0.97      0.96       474\n",
      "           9       0.97      0.96      0.97       472\n",
      "\n",
      "    accuracy                           0.97      4800\n",
      "   macro avg       0.97      0.97      0.97      4800\n",
      "weighted avg       0.97      0.97      0.97      4800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pred_rf_cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.99      0.98       415\n",
      "         1.0       0.98      0.99      0.99       523\n",
      "         2.0       0.96      0.97      0.97       516\n",
      "         3.0       0.98      0.96      0.97       494\n",
      "         4.0       0.97      0.98      0.97       444\n",
      "         5.0       0.98      0.96      0.97       430\n",
      "         6.0       0.98      0.99      0.98       494\n",
      "         7.0       0.99      0.97      0.98       538\n",
      "         8.0       0.96      0.95      0.95       474\n",
      "         9.0       0.97      0.97      0.97       472\n",
      "\n",
      "    accuracy                           0.97      4800\n",
      "   macro avg       0.97      0.97      0.97      4800\n",
      "weighted avg       0.97      0.97      0.97      4800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pred_xgrb_cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_dt</th>\n",
       "      <th>param_dt__max_iter</th>\n",
       "      <th>param_dt__solver</th>\n",
       "      <th>param_scaler__with_std</th>\n",
       "      <th>param_scaler</th>\n",
       "      <th>param_scaler__norm</th>\n",
       "      <th>...</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split8_test_score</th>\n",
       "      <th>split9_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90.217180</td>\n",
       "      <td>2.355714</td>\n",
       "      <td>0.185019</td>\n",
       "      <td>0.028745</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>150</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.914658</td>\n",
       "      <td>0.913752</td>\n",
       "      <td>0.917920</td>\n",
       "      <td>0.907411</td>\n",
       "      <td>0.907773</td>\n",
       "      <td>0.911941</td>\n",
       "      <td>0.914296</td>\n",
       "      <td>0.913520</td>\n",
       "      <td>0.003941</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.321292</td>\n",
       "      <td>1.530569</td>\n",
       "      <td>0.143300</td>\n",
       "      <td>0.020244</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>150</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.917014</td>\n",
       "      <td>0.918282</td>\n",
       "      <td>0.913571</td>\n",
       "      <td>0.906142</td>\n",
       "      <td>0.911216</td>\n",
       "      <td>0.911035</td>\n",
       "      <td>0.911035</td>\n",
       "      <td>0.913321</td>\n",
       "      <td>0.003813</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1013.794304</td>\n",
       "      <td>1.107258</td>\n",
       "      <td>0.118059</td>\n",
       "      <td>0.014571</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>150</td>\n",
       "      <td>saga</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.919551</td>\n",
       "      <td>0.921906</td>\n",
       "      <td>0.921000</td>\n",
       "      <td>0.920094</td>\n",
       "      <td>0.917558</td>\n",
       "      <td>0.913752</td>\n",
       "      <td>0.917376</td>\n",
       "      <td>0.919101</td>\n",
       "      <td>0.003368</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1010.589540</td>\n",
       "      <td>2.269747</td>\n",
       "      <td>0.092269</td>\n",
       "      <td>0.016088</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>150</td>\n",
       "      <td>saga</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.919732</td>\n",
       "      <td>0.917739</td>\n",
       "      <td>0.916470</td>\n",
       "      <td>0.909585</td>\n",
       "      <td>0.912303</td>\n",
       "      <td>0.912847</td>\n",
       "      <td>0.913571</td>\n",
       "      <td>0.915332</td>\n",
       "      <td>0.003828</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65.986691</td>\n",
       "      <td>9.765787</td>\n",
       "      <td>0.180859</td>\n",
       "      <td>0.012702</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>150</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Normalizer(copy=True, norm='max')</td>\n",
       "      <td>l1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.846530</td>\n",
       "      <td>0.849067</td>\n",
       "      <td>0.848161</td>\n",
       "      <td>0.844899</td>\n",
       "      <td>0.842725</td>\n",
       "      <td>0.843993</td>\n",
       "      <td>0.843269</td>\n",
       "      <td>0.845865</td>\n",
       "      <td>0.003031</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>87.381398</td>\n",
       "      <td>2.162852</td>\n",
       "      <td>0.143649</td>\n",
       "      <td>0.014892</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>150</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Normalizer(copy=True, norm='max')</td>\n",
       "      <td>l2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.917014</td>\n",
       "      <td>0.920275</td>\n",
       "      <td>0.917014</td>\n",
       "      <td>0.914840</td>\n",
       "      <td>0.915021</td>\n",
       "      <td>0.914115</td>\n",
       "      <td>0.919369</td>\n",
       "      <td>0.916618</td>\n",
       "      <td>0.003215</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>82.753476</td>\n",
       "      <td>3.413176</td>\n",
       "      <td>0.137027</td>\n",
       "      <td>0.021687</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>150</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Normalizer(copy=True, norm='max')</td>\n",
       "      <td>max</td>\n",
       "      <td>...</td>\n",
       "      <td>0.921725</td>\n",
       "      <td>0.924805</td>\n",
       "      <td>0.923174</td>\n",
       "      <td>0.917558</td>\n",
       "      <td>0.917376</td>\n",
       "      <td>0.918101</td>\n",
       "      <td>0.921000</td>\n",
       "      <td>0.920985</td>\n",
       "      <td>0.003075</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>107.309760</td>\n",
       "      <td>10.507532</td>\n",
       "      <td>0.125974</td>\n",
       "      <td>0.009646</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>150</td>\n",
       "      <td>saga</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Normalizer(copy=True, norm='max')</td>\n",
       "      <td>l1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.846349</td>\n",
       "      <td>0.849067</td>\n",
       "      <td>0.848161</td>\n",
       "      <td>0.844899</td>\n",
       "      <td>0.842906</td>\n",
       "      <td>0.843812</td>\n",
       "      <td>0.843450</td>\n",
       "      <td>0.845811</td>\n",
       "      <td>0.003101</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>139.987535</td>\n",
       "      <td>4.830789</td>\n",
       "      <td>0.092269</td>\n",
       "      <td>0.007036</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>150</td>\n",
       "      <td>saga</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Normalizer(copy=True, norm='max')</td>\n",
       "      <td>l2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.917195</td>\n",
       "      <td>0.920275</td>\n",
       "      <td>0.917014</td>\n",
       "      <td>0.915021</td>\n",
       "      <td>0.914477</td>\n",
       "      <td>0.913752</td>\n",
       "      <td>0.919551</td>\n",
       "      <td>0.916546</td>\n",
       "      <td>0.003313</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>987.057152</td>\n",
       "      <td>54.402229</td>\n",
       "      <td>0.086623</td>\n",
       "      <td>0.004338</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>150</td>\n",
       "      <td>saga</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Normalizer(copy=True, norm='max')</td>\n",
       "      <td>max</td>\n",
       "      <td>...</td>\n",
       "      <td>0.922269</td>\n",
       "      <td>0.923537</td>\n",
       "      <td>0.923899</td>\n",
       "      <td>0.916833</td>\n",
       "      <td>0.919188</td>\n",
       "      <td>0.917920</td>\n",
       "      <td>0.921363</td>\n",
       "      <td>0.921057</td>\n",
       "      <td>0.003365</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      90.217180      2.355714         0.185019        0.028745   \n",
       "1      88.321292      1.530569         0.143300        0.020244   \n",
       "2    1013.794304      1.107258         0.118059        0.014571   \n",
       "3    1010.589540      2.269747         0.092269        0.016088   \n",
       "4      65.986691      9.765787         0.180859        0.012702   \n",
       "5      87.381398      2.162852         0.143649        0.014892   \n",
       "6      82.753476      3.413176         0.137027        0.021687   \n",
       "7     107.309760     10.507532         0.125974        0.009646   \n",
       "8     139.987535      4.830789         0.092269        0.007036   \n",
       "9     987.057152     54.402229         0.086623        0.004338   \n",
       "\n",
       "                                            param_dt param_dt__max_iter  \\\n",
       "0  LogisticRegression(C=1.0, class_weight=None, d...                150   \n",
       "1  LogisticRegression(C=1.0, class_weight=None, d...                150   \n",
       "2  LogisticRegression(C=1.0, class_weight=None, d...                150   \n",
       "3  LogisticRegression(C=1.0, class_weight=None, d...                150   \n",
       "4  LogisticRegression(C=1.0, class_weight=None, d...                150   \n",
       "5  LogisticRegression(C=1.0, class_weight=None, d...                150   \n",
       "6  LogisticRegression(C=1.0, class_weight=None, d...                150   \n",
       "7  LogisticRegression(C=1.0, class_weight=None, d...                150   \n",
       "8  LogisticRegression(C=1.0, class_weight=None, d...                150   \n",
       "9  LogisticRegression(C=1.0, class_weight=None, d...                150   \n",
       "\n",
       "  param_dt__solver param_scaler__with_std                       param_scaler  \\\n",
       "0            lbfgs                   True                                NaN   \n",
       "1            lbfgs                  False                                NaN   \n",
       "2             saga                   True                                NaN   \n",
       "3             saga                  False                                NaN   \n",
       "4            lbfgs                    NaN  Normalizer(copy=True, norm='max')   \n",
       "5            lbfgs                    NaN  Normalizer(copy=True, norm='max')   \n",
       "6            lbfgs                    NaN  Normalizer(copy=True, norm='max')   \n",
       "7             saga                    NaN  Normalizer(copy=True, norm='max')   \n",
       "8             saga                    NaN  Normalizer(copy=True, norm='max')   \n",
       "9             saga                    NaN  Normalizer(copy=True, norm='max')   \n",
       "\n",
       "  param_scaler__norm  ... split3_test_score  split4_test_score  \\\n",
       "0                NaN  ...          0.914658           0.913752   \n",
       "1                NaN  ...          0.917014           0.918282   \n",
       "2                NaN  ...          0.919551           0.921906   \n",
       "3                NaN  ...          0.919732           0.917739   \n",
       "4                 l1  ...          0.846530           0.849067   \n",
       "5                 l2  ...          0.917014           0.920275   \n",
       "6                max  ...          0.921725           0.924805   \n",
       "7                 l1  ...          0.846349           0.849067   \n",
       "8                 l2  ...          0.917195           0.920275   \n",
       "9                max  ...          0.922269           0.923537   \n",
       "\n",
       "   split5_test_score  split6_test_score  split7_test_score  split8_test_score  \\\n",
       "0           0.917920           0.907411           0.907773           0.911941   \n",
       "1           0.913571           0.906142           0.911216           0.911035   \n",
       "2           0.921000           0.920094           0.917558           0.913752   \n",
       "3           0.916470           0.909585           0.912303           0.912847   \n",
       "4           0.848161           0.844899           0.842725           0.843993   \n",
       "5           0.917014           0.914840           0.915021           0.914115   \n",
       "6           0.923174           0.917558           0.917376           0.918101   \n",
       "7           0.848161           0.844899           0.842906           0.843812   \n",
       "8           0.917014           0.915021           0.914477           0.913752   \n",
       "9           0.923899           0.916833           0.919188           0.917920   \n",
       "\n",
       "   split9_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0           0.914296         0.913520        0.003941                7  \n",
       "1           0.911035         0.913321        0.003813                8  \n",
       "2           0.917376         0.919101        0.003368                3  \n",
       "3           0.913571         0.915332        0.003828                6  \n",
       "4           0.843269         0.845865        0.003031                9  \n",
       "5           0.919369         0.916618        0.003215                4  \n",
       "6           0.921000         0.920985        0.003075                2  \n",
       "7           0.843450         0.845811        0.003101               10  \n",
       "8           0.919551         0.916546        0.003313                5  \n",
       "9           0.921363         0.921057        0.003365                1  \n",
       "\n",
       "[10 rows x 24 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(lr_model.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_dt</th>\n",
       "      <th>param_dt__max_depth</th>\n",
       "      <th>param_scaler__with_std</th>\n",
       "      <th>param_scaler</th>\n",
       "      <th>param_scaler__norm</th>\n",
       "      <th>param_scaler__threshold</th>\n",
       "      <th>...</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split8_test_score</th>\n",
       "      <th>split9_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.326986</td>\n",
       "      <td>0.213278</td>\n",
       "      <td>0.175703</td>\n",
       "      <td>0.010631</td>\n",
       "      <td>DecisionTreeClassifier(ccp_alpha=0.0, class_we...</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.677659</td>\n",
       "      <td>0.675122</td>\n",
       "      <td>0.673854</td>\n",
       "      <td>0.670411</td>\n",
       "      <td>0.666063</td>\n",
       "      <td>0.674035</td>\n",
       "      <td>0.686537</td>\n",
       "      <td>0.674718</td>\n",
       "      <td>0.006881</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.143396</td>\n",
       "      <td>0.295196</td>\n",
       "      <td>0.127115</td>\n",
       "      <td>0.029036</td>\n",
       "      <td>DecisionTreeClassifier(ccp_alpha=0.0, class_we...</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.677659</td>\n",
       "      <td>0.675122</td>\n",
       "      <td>0.673854</td>\n",
       "      <td>0.670411</td>\n",
       "      <td>0.666063</td>\n",
       "      <td>0.674035</td>\n",
       "      <td>0.686537</td>\n",
       "      <td>0.674718</td>\n",
       "      <td>0.006881</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.038673</td>\n",
       "      <td>0.502816</td>\n",
       "      <td>0.176380</td>\n",
       "      <td>0.045046</td>\n",
       "      <td>DecisionTreeClassifier(ccp_alpha=0.0, class_we...</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.734010</td>\n",
       "      <td>0.738540</td>\n",
       "      <td>0.726762</td>\n",
       "      <td>0.718971</td>\n",
       "      <td>0.725131</td>\n",
       "      <td>0.727849</td>\n",
       "      <td>0.735097</td>\n",
       "      <td>0.731718</td>\n",
       "      <td>0.006327</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.102826</td>\n",
       "      <td>1.150246</td>\n",
       "      <td>0.131764</td>\n",
       "      <td>0.027528</td>\n",
       "      <td>DecisionTreeClassifier(ccp_alpha=0.0, class_we...</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.734010</td>\n",
       "      <td>0.738540</td>\n",
       "      <td>0.726762</td>\n",
       "      <td>0.719152</td>\n",
       "      <td>0.724950</td>\n",
       "      <td>0.727668</td>\n",
       "      <td>0.734916</td>\n",
       "      <td>0.731664</td>\n",
       "      <td>0.006289</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23.289003</td>\n",
       "      <td>0.268028</td>\n",
       "      <td>0.184423</td>\n",
       "      <td>0.038917</td>\n",
       "      <td>DecisionTreeClassifier(ccp_alpha=0.0, class_we...</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.838014</td>\n",
       "      <td>0.840732</td>\n",
       "      <td>0.838558</td>\n",
       "      <td>0.831310</td>\n",
       "      <td>0.834934</td>\n",
       "      <td>0.832035</td>\n",
       "      <td>0.831491</td>\n",
       "      <td>0.836081</td>\n",
       "      <td>0.003635</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21.584324</td>\n",
       "      <td>0.643064</td>\n",
       "      <td>0.146398</td>\n",
       "      <td>0.019634</td>\n",
       "      <td>DecisionTreeClassifier(ccp_alpha=0.0, class_we...</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.839464</td>\n",
       "      <td>0.838920</td>\n",
       "      <td>0.838377</td>\n",
       "      <td>0.830766</td>\n",
       "      <td>0.835296</td>\n",
       "      <td>0.831491</td>\n",
       "      <td>0.831129</td>\n",
       "      <td>0.835973</td>\n",
       "      <td>0.003921</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18.197492</td>\n",
       "      <td>0.081825</td>\n",
       "      <td>0.207178</td>\n",
       "      <td>0.033675</td>\n",
       "      <td>DecisionTreeClassifier(ccp_alpha=0.0, class_we...</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Normalizer(copy=True, norm='l2')</td>\n",
       "      <td>l1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.658996</td>\n",
       "      <td>0.664251</td>\n",
       "      <td>0.659177</td>\n",
       "      <td>0.660989</td>\n",
       "      <td>0.657909</td>\n",
       "      <td>0.659902</td>\n",
       "      <td>0.664613</td>\n",
       "      <td>0.661799</td>\n",
       "      <td>0.003643</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>17.856472</td>\n",
       "      <td>0.234431</td>\n",
       "      <td>0.180009</td>\n",
       "      <td>0.032666</td>\n",
       "      <td>DecisionTreeClassifier(ccp_alpha=0.0, class_we...</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Normalizer(copy=True, norm='l2')</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.657547</td>\n",
       "      <td>0.661533</td>\n",
       "      <td>0.659177</td>\n",
       "      <td>0.661171</td>\n",
       "      <td>0.658090</td>\n",
       "      <td>0.660989</td>\n",
       "      <td>0.664432</td>\n",
       "      <td>0.661581</td>\n",
       "      <td>0.004180</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13.626702</td>\n",
       "      <td>0.085149</td>\n",
       "      <td>0.162740</td>\n",
       "      <td>0.029647</td>\n",
       "      <td>DecisionTreeClassifier(ccp_alpha=0.0, class_we...</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Normalizer(copy=True, norm='l2')</td>\n",
       "      <td>max</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.677115</td>\n",
       "      <td>0.674941</td>\n",
       "      <td>0.673854</td>\n",
       "      <td>0.670592</td>\n",
       "      <td>0.666063</td>\n",
       "      <td>0.674035</td>\n",
       "      <td>0.686537</td>\n",
       "      <td>0.674446</td>\n",
       "      <td>0.007259</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20.765401</td>\n",
       "      <td>0.365100</td>\n",
       "      <td>0.189692</td>\n",
       "      <td>0.013224</td>\n",
       "      <td>DecisionTreeClassifier(ccp_alpha=0.0, class_we...</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Normalizer(copy=True, norm='l2')</td>\n",
       "      <td>l1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.714622</td>\n",
       "      <td>0.721870</td>\n",
       "      <td>0.716978</td>\n",
       "      <td>0.715528</td>\n",
       "      <td>0.710274</td>\n",
       "      <td>0.720783</td>\n",
       "      <td>0.718608</td>\n",
       "      <td>0.717948</td>\n",
       "      <td>0.005129</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20.222587</td>\n",
       "      <td>0.113908</td>\n",
       "      <td>0.137560</td>\n",
       "      <td>0.019755</td>\n",
       "      <td>DecisionTreeClassifier(ccp_alpha=0.0, class_we...</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Normalizer(copy=True, norm='l2')</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.715347</td>\n",
       "      <td>0.720239</td>\n",
       "      <td>0.717340</td>\n",
       "      <td>0.713173</td>\n",
       "      <td>0.711723</td>\n",
       "      <td>0.720783</td>\n",
       "      <td>0.720239</td>\n",
       "      <td>0.718637</td>\n",
       "      <td>0.004858</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>15.482545</td>\n",
       "      <td>0.478441</td>\n",
       "      <td>0.131157</td>\n",
       "      <td>0.024905</td>\n",
       "      <td>DecisionTreeClassifier(ccp_alpha=0.0, class_we...</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Normalizer(copy=True, norm='l2')</td>\n",
       "      <td>max</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.733829</td>\n",
       "      <td>0.738177</td>\n",
       "      <td>0.726762</td>\n",
       "      <td>0.719877</td>\n",
       "      <td>0.724950</td>\n",
       "      <td>0.728212</td>\n",
       "      <td>0.734916</td>\n",
       "      <td>0.731736</td>\n",
       "      <td>0.006058</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>27.241467</td>\n",
       "      <td>0.138899</td>\n",
       "      <td>0.175767</td>\n",
       "      <td>0.027655</td>\n",
       "      <td>DecisionTreeClassifier(ccp_alpha=0.0, class_we...</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Normalizer(copy=True, norm='l2')</td>\n",
       "      <td>l1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.830585</td>\n",
       "      <td>0.826780</td>\n",
       "      <td>0.826599</td>\n",
       "      <td>0.821344</td>\n",
       "      <td>0.822069</td>\n",
       "      <td>0.825874</td>\n",
       "      <td>0.818808</td>\n",
       "      <td>0.824214</td>\n",
       "      <td>0.004115</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>26.851969</td>\n",
       "      <td>0.062957</td>\n",
       "      <td>0.134550</td>\n",
       "      <td>0.018727</td>\n",
       "      <td>DecisionTreeClassifier(ccp_alpha=0.0, class_we...</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Normalizer(copy=True, norm='l2')</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.828592</td>\n",
       "      <td>0.829679</td>\n",
       "      <td>0.829679</td>\n",
       "      <td>0.820982</td>\n",
       "      <td>0.823338</td>\n",
       "      <td>0.826055</td>\n",
       "      <td>0.821707</td>\n",
       "      <td>0.826533</td>\n",
       "      <td>0.003666</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20.108842</td>\n",
       "      <td>0.298860</td>\n",
       "      <td>0.141158</td>\n",
       "      <td>0.022250</td>\n",
       "      <td>DecisionTreeClassifier(ccp_alpha=0.0, class_we...</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Normalizer(copy=True, norm='l2')</td>\n",
       "      <td>max</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.840551</td>\n",
       "      <td>0.842725</td>\n",
       "      <td>0.838920</td>\n",
       "      <td>0.831310</td>\n",
       "      <td>0.835477</td>\n",
       "      <td>0.834028</td>\n",
       "      <td>0.830404</td>\n",
       "      <td>0.836824</td>\n",
       "      <td>0.004212</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>11.403558</td>\n",
       "      <td>0.743259</td>\n",
       "      <td>0.072574</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>DecisionTreeClassifier(ccp_alpha=0.0, class_we...</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Binarizer(copy=True, threshold=18.257908978953...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>174.258</td>\n",
       "      <td>...</td>\n",
       "      <td>0.590143</td>\n",
       "      <td>0.585432</td>\n",
       "      <td>0.584889</td>\n",
       "      <td>0.573111</td>\n",
       "      <td>0.586700</td>\n",
       "      <td>0.574379</td>\n",
       "      <td>0.578366</td>\n",
       "      <td>0.582675</td>\n",
       "      <td>0.006273</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10.602775</td>\n",
       "      <td>0.621366</td>\n",
       "      <td>0.072253</td>\n",
       "      <td>0.007907</td>\n",
       "      <td>DecisionTreeClassifier(ccp_alpha=0.0, class_we...</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Binarizer(copy=True, threshold=18.257908978953...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.357673</td>\n",
       "      <td>0.378148</td>\n",
       "      <td>0.380504</td>\n",
       "      <td>0.372712</td>\n",
       "      <td>0.374343</td>\n",
       "      <td>0.364921</td>\n",
       "      <td>0.380866</td>\n",
       "      <td>0.373931</td>\n",
       "      <td>0.007472</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>11.776809</td>\n",
       "      <td>0.567577</td>\n",
       "      <td>0.074297</td>\n",
       "      <td>0.004496</td>\n",
       "      <td>DecisionTreeClassifier(ccp_alpha=0.0, class_we...</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Binarizer(copy=True, threshold=18.257908978953...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.2579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.681464</td>\n",
       "      <td>0.681464</td>\n",
       "      <td>0.685269</td>\n",
       "      <td>0.676572</td>\n",
       "      <td>0.680196</td>\n",
       "      <td>0.678746</td>\n",
       "      <td>0.675122</td>\n",
       "      <td>0.677417</td>\n",
       "      <td>0.005323</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>12.671023</td>\n",
       "      <td>0.630596</td>\n",
       "      <td>0.081245</td>\n",
       "      <td>0.007211</td>\n",
       "      <td>DecisionTreeClassifier(ccp_alpha=0.0, class_we...</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Binarizer(copy=True, threshold=18.257908978953...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>174.258</td>\n",
       "      <td>...</td>\n",
       "      <td>0.634716</td>\n",
       "      <td>0.624570</td>\n",
       "      <td>0.637978</td>\n",
       "      <td>0.623664</td>\n",
       "      <td>0.631636</td>\n",
       "      <td>0.620221</td>\n",
       "      <td>0.621852</td>\n",
       "      <td>0.628352</td>\n",
       "      <td>0.005729</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>11.922008</td>\n",
       "      <td>1.000719</td>\n",
       "      <td>0.068198</td>\n",
       "      <td>0.007948</td>\n",
       "      <td>DecisionTreeClassifier(ccp_alpha=0.0, class_we...</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Binarizer(copy=True, threshold=18.257908978953...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.405327</td>\n",
       "      <td>0.406052</td>\n",
       "      <td>0.398804</td>\n",
       "      <td>0.390469</td>\n",
       "      <td>0.390650</td>\n",
       "      <td>0.413481</td>\n",
       "      <td>0.400797</td>\n",
       "      <td>0.399660</td>\n",
       "      <td>0.007550</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>13.819640</td>\n",
       "      <td>0.937828</td>\n",
       "      <td>0.075295</td>\n",
       "      <td>0.009120</td>\n",
       "      <td>DecisionTreeClassifier(ccp_alpha=0.0, class_we...</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Binarizer(copy=True, threshold=18.257908978953...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.2579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.739083</td>\n",
       "      <td>0.745244</td>\n",
       "      <td>0.742707</td>\n",
       "      <td>0.734372</td>\n",
       "      <td>0.741439</td>\n",
       "      <td>0.733104</td>\n",
       "      <td>0.736728</td>\n",
       "      <td>0.737625</td>\n",
       "      <td>0.005896</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>16.449567</td>\n",
       "      <td>2.056133</td>\n",
       "      <td>0.072498</td>\n",
       "      <td>0.002578</td>\n",
       "      <td>DecisionTreeClassifier(ccp_alpha=0.0, class_we...</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Binarizer(copy=True, threshold=18.257908978953...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>174.258</td>\n",
       "      <td>...</td>\n",
       "      <td>0.750498</td>\n",
       "      <td>0.744881</td>\n",
       "      <td>0.755934</td>\n",
       "      <td>0.743975</td>\n",
       "      <td>0.748505</td>\n",
       "      <td>0.745968</td>\n",
       "      <td>0.737996</td>\n",
       "      <td>0.748043</td>\n",
       "      <td>0.004914</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>16.179233</td>\n",
       "      <td>1.708795</td>\n",
       "      <td>0.069096</td>\n",
       "      <td>0.006874</td>\n",
       "      <td>DecisionTreeClassifier(ccp_alpha=0.0, class_we...</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Binarizer(copy=True, threshold=18.257908978953...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.477442</td>\n",
       "      <td>0.481609</td>\n",
       "      <td>0.503714</td>\n",
       "      <td>0.485595</td>\n",
       "      <td>0.480703</td>\n",
       "      <td>0.482515</td>\n",
       "      <td>0.486864</td>\n",
       "      <td>0.485143</td>\n",
       "      <td>0.007184</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>15.815683</td>\n",
       "      <td>1.476587</td>\n",
       "      <td>0.077028</td>\n",
       "      <td>0.011805</td>\n",
       "      <td>DecisionTreeClassifier(ccp_alpha=0.0, class_we...</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Binarizer(copy=True, threshold=18.257908978953...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.2579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.847255</td>\n",
       "      <td>0.854321</td>\n",
       "      <td>0.853778</td>\n",
       "      <td>0.843450</td>\n",
       "      <td>0.847617</td>\n",
       "      <td>0.840732</td>\n",
       "      <td>0.844537</td>\n",
       "      <td>0.845811</td>\n",
       "      <td>0.004669</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       17.326986      0.213278         0.175703        0.010631   \n",
       "1       15.143396      0.295196         0.127115        0.029036   \n",
       "2       19.038673      0.502816         0.176380        0.045046   \n",
       "3       17.102826      1.150246         0.131764        0.027528   \n",
       "4       23.289003      0.268028         0.184423        0.038917   \n",
       "5       21.584324      0.643064         0.146398        0.019634   \n",
       "6       18.197492      0.081825         0.207178        0.033675   \n",
       "7       17.856472      0.234431         0.180009        0.032666   \n",
       "8       13.626702      0.085149         0.162740        0.029647   \n",
       "9       20.765401      0.365100         0.189692        0.013224   \n",
       "10      20.222587      0.113908         0.137560        0.019755   \n",
       "11      15.482545      0.478441         0.131157        0.024905   \n",
       "12      27.241467      0.138899         0.175767        0.027655   \n",
       "13      26.851969      0.062957         0.134550        0.018727   \n",
       "14      20.108842      0.298860         0.141158        0.022250   \n",
       "15      11.403558      0.743259         0.072574        0.004136   \n",
       "16      10.602775      0.621366         0.072253        0.007907   \n",
       "17      11.776809      0.567577         0.074297        0.004496   \n",
       "18      12.671023      0.630596         0.081245        0.007211   \n",
       "19      11.922008      1.000719         0.068198        0.007948   \n",
       "20      13.819640      0.937828         0.075295        0.009120   \n",
       "21      16.449567      2.056133         0.072498        0.002578   \n",
       "22      16.179233      1.708795         0.069096        0.006874   \n",
       "23      15.815683      1.476587         0.077028        0.011805   \n",
       "\n",
       "                                             param_dt param_dt__max_depth  \\\n",
       "0   DecisionTreeClassifier(ccp_alpha=0.0, class_we...                   5   \n",
       "1   DecisionTreeClassifier(ccp_alpha=0.0, class_we...                   5   \n",
       "2   DecisionTreeClassifier(ccp_alpha=0.0, class_we...                   6   \n",
       "3   DecisionTreeClassifier(ccp_alpha=0.0, class_we...                   6   \n",
       "4   DecisionTreeClassifier(ccp_alpha=0.0, class_we...                   9   \n",
       "5   DecisionTreeClassifier(ccp_alpha=0.0, class_we...                   9   \n",
       "6   DecisionTreeClassifier(ccp_alpha=0.0, class_we...                   5   \n",
       "7   DecisionTreeClassifier(ccp_alpha=0.0, class_we...                   5   \n",
       "8   DecisionTreeClassifier(ccp_alpha=0.0, class_we...                   5   \n",
       "9   DecisionTreeClassifier(ccp_alpha=0.0, class_we...                   6   \n",
       "10  DecisionTreeClassifier(ccp_alpha=0.0, class_we...                   6   \n",
       "11  DecisionTreeClassifier(ccp_alpha=0.0, class_we...                   6   \n",
       "12  DecisionTreeClassifier(ccp_alpha=0.0, class_we...                   9   \n",
       "13  DecisionTreeClassifier(ccp_alpha=0.0, class_we...                   9   \n",
       "14  DecisionTreeClassifier(ccp_alpha=0.0, class_we...                   9   \n",
       "15  DecisionTreeClassifier(ccp_alpha=0.0, class_we...                   5   \n",
       "16  DecisionTreeClassifier(ccp_alpha=0.0, class_we...                   5   \n",
       "17  DecisionTreeClassifier(ccp_alpha=0.0, class_we...                   5   \n",
       "18  DecisionTreeClassifier(ccp_alpha=0.0, class_we...                   6   \n",
       "19  DecisionTreeClassifier(ccp_alpha=0.0, class_we...                   6   \n",
       "20  DecisionTreeClassifier(ccp_alpha=0.0, class_we...                   6   \n",
       "21  DecisionTreeClassifier(ccp_alpha=0.0, class_we...                   9   \n",
       "22  DecisionTreeClassifier(ccp_alpha=0.0, class_we...                   9   \n",
       "23  DecisionTreeClassifier(ccp_alpha=0.0, class_we...                   9   \n",
       "\n",
       "   param_scaler__with_std                                       param_scaler  \\\n",
       "0                    True                                                NaN   \n",
       "1                   False                                                NaN   \n",
       "2                    True                                                NaN   \n",
       "3                   False                                                NaN   \n",
       "4                    True                                                NaN   \n",
       "5                   False                                                NaN   \n",
       "6                     NaN                   Normalizer(copy=True, norm='l2')   \n",
       "7                     NaN                   Normalizer(copy=True, norm='l2')   \n",
       "8                     NaN                   Normalizer(copy=True, norm='l2')   \n",
       "9                     NaN                   Normalizer(copy=True, norm='l2')   \n",
       "10                    NaN                   Normalizer(copy=True, norm='l2')   \n",
       "11                    NaN                   Normalizer(copy=True, norm='l2')   \n",
       "12                    NaN                   Normalizer(copy=True, norm='l2')   \n",
       "13                    NaN                   Normalizer(copy=True, norm='l2')   \n",
       "14                    NaN                   Normalizer(copy=True, norm='l2')   \n",
       "15                    NaN  Binarizer(copy=True, threshold=18.257908978953...   \n",
       "16                    NaN  Binarizer(copy=True, threshold=18.257908978953...   \n",
       "17                    NaN  Binarizer(copy=True, threshold=18.257908978953...   \n",
       "18                    NaN  Binarizer(copy=True, threshold=18.257908978953...   \n",
       "19                    NaN  Binarizer(copy=True, threshold=18.257908978953...   \n",
       "20                    NaN  Binarizer(copy=True, threshold=18.257908978953...   \n",
       "21                    NaN  Binarizer(copy=True, threshold=18.257908978953...   \n",
       "22                    NaN  Binarizer(copy=True, threshold=18.257908978953...   \n",
       "23                    NaN  Binarizer(copy=True, threshold=18.257908978953...   \n",
       "\n",
       "   param_scaler__norm param_scaler__threshold  ... split3_test_score  \\\n",
       "0                 NaN                     NaN  ...          0.677659   \n",
       "1                 NaN                     NaN  ...          0.677659   \n",
       "2                 NaN                     NaN  ...          0.734010   \n",
       "3                 NaN                     NaN  ...          0.734010   \n",
       "4                 NaN                     NaN  ...          0.838014   \n",
       "5                 NaN                     NaN  ...          0.839464   \n",
       "6                  l1                     NaN  ...          0.658996   \n",
       "7                  l2                     NaN  ...          0.657547   \n",
       "8                 max                     NaN  ...          0.677115   \n",
       "9                  l1                     NaN  ...          0.714622   \n",
       "10                 l2                     NaN  ...          0.715347   \n",
       "11                max                     NaN  ...          0.733829   \n",
       "12                 l1                     NaN  ...          0.830585   \n",
       "13                 l2                     NaN  ...          0.828592   \n",
       "14                max                     NaN  ...          0.840551   \n",
       "15                NaN                 174.258  ...          0.590143   \n",
       "16                NaN                     252  ...          0.357673   \n",
       "17                NaN                 18.2579  ...          0.681464   \n",
       "18                NaN                 174.258  ...          0.634716   \n",
       "19                NaN                     252  ...          0.405327   \n",
       "20                NaN                 18.2579  ...          0.739083   \n",
       "21                NaN                 174.258  ...          0.750498   \n",
       "22                NaN                     252  ...          0.477442   \n",
       "23                NaN                 18.2579  ...          0.847255   \n",
       "\n",
       "    split4_test_score  split5_test_score  split6_test_score  \\\n",
       "0            0.675122           0.673854           0.670411   \n",
       "1            0.675122           0.673854           0.670411   \n",
       "2            0.738540           0.726762           0.718971   \n",
       "3            0.738540           0.726762           0.719152   \n",
       "4            0.840732           0.838558           0.831310   \n",
       "5            0.838920           0.838377           0.830766   \n",
       "6            0.664251           0.659177           0.660989   \n",
       "7            0.661533           0.659177           0.661171   \n",
       "8            0.674941           0.673854           0.670592   \n",
       "9            0.721870           0.716978           0.715528   \n",
       "10           0.720239           0.717340           0.713173   \n",
       "11           0.738177           0.726762           0.719877   \n",
       "12           0.826780           0.826599           0.821344   \n",
       "13           0.829679           0.829679           0.820982   \n",
       "14           0.842725           0.838920           0.831310   \n",
       "15           0.585432           0.584889           0.573111   \n",
       "16           0.378148           0.380504           0.372712   \n",
       "17           0.681464           0.685269           0.676572   \n",
       "18           0.624570           0.637978           0.623664   \n",
       "19           0.406052           0.398804           0.390469   \n",
       "20           0.745244           0.742707           0.734372   \n",
       "21           0.744881           0.755934           0.743975   \n",
       "22           0.481609           0.503714           0.485595   \n",
       "23           0.854321           0.853778           0.843450   \n",
       "\n",
       "    split7_test_score  split8_test_score  split9_test_score  mean_test_score  \\\n",
       "0            0.666063           0.674035           0.686537         0.674718   \n",
       "1            0.666063           0.674035           0.686537         0.674718   \n",
       "2            0.725131           0.727849           0.735097         0.731718   \n",
       "3            0.724950           0.727668           0.734916         0.731664   \n",
       "4            0.834934           0.832035           0.831491         0.836081   \n",
       "5            0.835296           0.831491           0.831129         0.835973   \n",
       "6            0.657909           0.659902           0.664613         0.661799   \n",
       "7            0.658090           0.660989           0.664432         0.661581   \n",
       "8            0.666063           0.674035           0.686537         0.674446   \n",
       "9            0.710274           0.720783           0.718608         0.717948   \n",
       "10           0.711723           0.720783           0.720239         0.718637   \n",
       "11           0.724950           0.728212           0.734916         0.731736   \n",
       "12           0.822069           0.825874           0.818808         0.824214   \n",
       "13           0.823338           0.826055           0.821707         0.826533   \n",
       "14           0.835477           0.834028           0.830404         0.836824   \n",
       "15           0.586700           0.574379           0.578366         0.582675   \n",
       "16           0.374343           0.364921           0.380866         0.373931   \n",
       "17           0.680196           0.678746           0.675122         0.677417   \n",
       "18           0.631636           0.620221           0.621852         0.628352   \n",
       "19           0.390650           0.413481           0.400797         0.399660   \n",
       "20           0.741439           0.733104           0.736728         0.737625   \n",
       "21           0.748505           0.745968           0.737996         0.748043   \n",
       "22           0.480703           0.482515           0.486864         0.485143   \n",
       "23           0.847617           0.840732           0.844537         0.845811   \n",
       "\n",
       "    std_test_score  rank_test_score  \n",
       "0         0.006881               15  \n",
       "1         0.006881               15  \n",
       "2         0.006327               10  \n",
       "3         0.006289               11  \n",
       "4         0.003635                3  \n",
       "5         0.003921                4  \n",
       "6         0.003643               18  \n",
       "7         0.004180               19  \n",
       "8         0.007259               17  \n",
       "9         0.005129               13  \n",
       "10        0.004858               12  \n",
       "11        0.006058                9  \n",
       "12        0.004115                6  \n",
       "13        0.003666                5  \n",
       "14        0.004212                2  \n",
       "15        0.006273               21  \n",
       "16        0.007472               24  \n",
       "17        0.005323               14  \n",
       "18        0.005729               20  \n",
       "19        0.007550               23  \n",
       "20        0.005896                8  \n",
       "21        0.004914                7  \n",
       "22        0.007184               22  \n",
       "23        0.004669                1  \n",
       "\n",
       "[24 rows x 24 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dt_model.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_dt</th>\n",
       "      <th>param_dt__bootstrap</th>\n",
       "      <th>param_dt__max_depth</th>\n",
       "      <th>param_dt__min_samples_leaf</th>\n",
       "      <th>param_dt__n_estimators</th>\n",
       "      <th>param_scaler__with_std</th>\n",
       "      <th>...</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split8_test_score</th>\n",
       "      <th>split9_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>376.445978</td>\n",
       "      <td>3.020293</td>\n",
       "      <td>3.167030</td>\n",
       "      <td>0.361219</td>\n",
       "      <td>RandomForestClassifier(bootstrap=False, ccp_al...</td>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.965755</td>\n",
       "      <td>0.968654</td>\n",
       "      <td>0.962131</td>\n",
       "      <td>0.963580</td>\n",
       "      <td>0.963762</td>\n",
       "      <td>0.968110</td>\n",
       "      <td>0.964305</td>\n",
       "      <td>0.965810</td>\n",
       "      <td>0.002267</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>375.676825</td>\n",
       "      <td>4.016717</td>\n",
       "      <td>2.966644</td>\n",
       "      <td>0.419878</td>\n",
       "      <td>RandomForestClassifier(bootstrap=False, ccp_al...</td>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.965030</td>\n",
       "      <td>0.969560</td>\n",
       "      <td>0.964305</td>\n",
       "      <td>0.963399</td>\n",
       "      <td>0.963580</td>\n",
       "      <td>0.969379</td>\n",
       "      <td>0.965392</td>\n",
       "      <td>0.965774</td>\n",
       "      <td>0.002256</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>613.868614</td>\n",
       "      <td>24.567814</td>\n",
       "      <td>4.498974</td>\n",
       "      <td>0.567725</td>\n",
       "      <td>RandomForestClassifier(bootstrap=False, ccp_al...</td>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>800</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.965392</td>\n",
       "      <td>0.967929</td>\n",
       "      <td>0.964124</td>\n",
       "      <td>0.964486</td>\n",
       "      <td>0.963218</td>\n",
       "      <td>0.967929</td>\n",
       "      <td>0.966479</td>\n",
       "      <td>0.965919</td>\n",
       "      <td>0.001937</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>598.148359</td>\n",
       "      <td>3.939247</td>\n",
       "      <td>4.154724</td>\n",
       "      <td>0.210071</td>\n",
       "      <td>RandomForestClassifier(bootstrap=False, ccp_al...</td>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>800</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.965573</td>\n",
       "      <td>0.968110</td>\n",
       "      <td>0.964849</td>\n",
       "      <td>0.964668</td>\n",
       "      <td>0.962493</td>\n",
       "      <td>0.968110</td>\n",
       "      <td>0.965211</td>\n",
       "      <td>0.965629</td>\n",
       "      <td>0.001715</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>362.483617</td>\n",
       "      <td>2.267863</td>\n",
       "      <td>2.484566</td>\n",
       "      <td>0.075116</td>\n",
       "      <td>RandomForestClassifier(bootstrap=False, ccp_al...</td>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.963218</td>\n",
       "      <td>0.965030</td>\n",
       "      <td>0.960138</td>\n",
       "      <td>0.962131</td>\n",
       "      <td>0.959051</td>\n",
       "      <td>0.964305</td>\n",
       "      <td>0.960862</td>\n",
       "      <td>0.962350</td>\n",
       "      <td>0.002067</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>362.358065</td>\n",
       "      <td>4.792959</td>\n",
       "      <td>2.441112</td>\n",
       "      <td>0.021331</td>\n",
       "      <td>RandomForestClassifier(bootstrap=False, ccp_al...</td>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.963399</td>\n",
       "      <td>0.965573</td>\n",
       "      <td>0.960138</td>\n",
       "      <td>0.960500</td>\n",
       "      <td>0.960681</td>\n",
       "      <td>0.965211</td>\n",
       "      <td>0.961768</td>\n",
       "      <td>0.962929</td>\n",
       "      <td>0.002121</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>579.947019</td>\n",
       "      <td>7.887904</td>\n",
       "      <td>3.883066</td>\n",
       "      <td>0.031040</td>\n",
       "      <td>RandomForestClassifier(bootstrap=False, ccp_al...</td>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>800</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.962312</td>\n",
       "      <td>0.964668</td>\n",
       "      <td>0.961406</td>\n",
       "      <td>0.961768</td>\n",
       "      <td>0.961225</td>\n",
       "      <td>0.966117</td>\n",
       "      <td>0.961406</td>\n",
       "      <td>0.962730</td>\n",
       "      <td>0.001603</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>575.249044</td>\n",
       "      <td>0.880367</td>\n",
       "      <td>3.883574</td>\n",
       "      <td>0.052808</td>\n",
       "      <td>RandomForestClassifier(bootstrap=False, ccp_al...</td>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>800</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.963762</td>\n",
       "      <td>0.965030</td>\n",
       "      <td>0.960500</td>\n",
       "      <td>0.961587</td>\n",
       "      <td>0.960500</td>\n",
       "      <td>0.965573</td>\n",
       "      <td>0.962312</td>\n",
       "      <td>0.963020</td>\n",
       "      <td>0.001828</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>376.094352</td>\n",
       "      <td>1.823592</td>\n",
       "      <td>2.520802</td>\n",
       "      <td>0.020453</td>\n",
       "      <td>RandomForestClassifier(bootstrap=False, ccp_al...</td>\n",
       "      <td>True</td>\n",
       "      <td>120</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.965030</td>\n",
       "      <td>0.968835</td>\n",
       "      <td>0.963580</td>\n",
       "      <td>0.963580</td>\n",
       "      <td>0.963943</td>\n",
       "      <td>0.967567</td>\n",
       "      <td>0.965936</td>\n",
       "      <td>0.965720</td>\n",
       "      <td>0.001865</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>374.463443</td>\n",
       "      <td>1.074614</td>\n",
       "      <td>2.502445</td>\n",
       "      <td>0.021487</td>\n",
       "      <td>RandomForestClassifier(bootstrap=False, ccp_al...</td>\n",
       "      <td>True</td>\n",
       "      <td>120</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.966298</td>\n",
       "      <td>0.968654</td>\n",
       "      <td>0.961950</td>\n",
       "      <td>0.963580</td>\n",
       "      <td>0.962312</td>\n",
       "      <td>0.967748</td>\n",
       "      <td>0.965936</td>\n",
       "      <td>0.965629</td>\n",
       "      <td>0.002392</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>598.061443</td>\n",
       "      <td>1.165268</td>\n",
       "      <td>4.024759</td>\n",
       "      <td>0.053699</td>\n",
       "      <td>RandomForestClassifier(bootstrap=False, ccp_al...</td>\n",
       "      <td>True</td>\n",
       "      <td>120</td>\n",
       "      <td>3</td>\n",
       "      <td>800</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.966661</td>\n",
       "      <td>0.968473</td>\n",
       "      <td>0.963037</td>\n",
       "      <td>0.963943</td>\n",
       "      <td>0.963580</td>\n",
       "      <td>0.968654</td>\n",
       "      <td>0.964124</td>\n",
       "      <td>0.965810</td>\n",
       "      <td>0.001974</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>596.099226</td>\n",
       "      <td>1.164400</td>\n",
       "      <td>3.983244</td>\n",
       "      <td>0.054331</td>\n",
       "      <td>RandomForestClassifier(bootstrap=False, ccp_al...</td>\n",
       "      <td>True</td>\n",
       "      <td>120</td>\n",
       "      <td>3</td>\n",
       "      <td>800</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.965755</td>\n",
       "      <td>0.968473</td>\n",
       "      <td>0.964305</td>\n",
       "      <td>0.965030</td>\n",
       "      <td>0.962493</td>\n",
       "      <td>0.968835</td>\n",
       "      <td>0.966117</td>\n",
       "      <td>0.965991</td>\n",
       "      <td>0.001895</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>362.536928</td>\n",
       "      <td>1.049495</td>\n",
       "      <td>2.450763</td>\n",
       "      <td>0.022499</td>\n",
       "      <td>RandomForestClassifier(bootstrap=False, ccp_al...</td>\n",
       "      <td>True</td>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.962493</td>\n",
       "      <td>0.965211</td>\n",
       "      <td>0.960681</td>\n",
       "      <td>0.961044</td>\n",
       "      <td>0.961225</td>\n",
       "      <td>0.965211</td>\n",
       "      <td>0.962131</td>\n",
       "      <td>0.962766</td>\n",
       "      <td>0.001610</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>360.920044</td>\n",
       "      <td>1.841710</td>\n",
       "      <td>2.435615</td>\n",
       "      <td>0.026228</td>\n",
       "      <td>RandomForestClassifier(bootstrap=False, ccp_al...</td>\n",
       "      <td>True</td>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.964124</td>\n",
       "      <td>0.966117</td>\n",
       "      <td>0.960500</td>\n",
       "      <td>0.960681</td>\n",
       "      <td>0.959594</td>\n",
       "      <td>0.965936</td>\n",
       "      <td>0.960138</td>\n",
       "      <td>0.962893</td>\n",
       "      <td>0.002528</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>576.560057</td>\n",
       "      <td>0.780648</td>\n",
       "      <td>3.857213</td>\n",
       "      <td>0.033624</td>\n",
       "      <td>RandomForestClassifier(bootstrap=False, ccp_al...</td>\n",
       "      <td>True</td>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>800</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.963037</td>\n",
       "      <td>0.965211</td>\n",
       "      <td>0.960862</td>\n",
       "      <td>0.960862</td>\n",
       "      <td>0.960681</td>\n",
       "      <td>0.965030</td>\n",
       "      <td>0.962312</td>\n",
       "      <td>0.962784</td>\n",
       "      <td>0.001689</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>575.936539</td>\n",
       "      <td>1.421310</td>\n",
       "      <td>3.862327</td>\n",
       "      <td>0.033132</td>\n",
       "      <td>RandomForestClassifier(bootstrap=False, ccp_al...</td>\n",
       "      <td>True</td>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>800</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.962131</td>\n",
       "      <td>0.965211</td>\n",
       "      <td>0.960862</td>\n",
       "      <td>0.960862</td>\n",
       "      <td>0.961044</td>\n",
       "      <td>0.966298</td>\n",
       "      <td>0.962131</td>\n",
       "      <td>0.962730</td>\n",
       "      <td>0.001942</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>616.162348</td>\n",
       "      <td>1.117404</td>\n",
       "      <td>2.670657</td>\n",
       "      <td>0.022961</td>\n",
       "      <td>RandomForestClassifier(bootstrap=False, ccp_al...</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.969741</td>\n",
       "      <td>0.973365</td>\n",
       "      <td>0.966842</td>\n",
       "      <td>0.966479</td>\n",
       "      <td>0.966842</td>\n",
       "      <td>0.971553</td>\n",
       "      <td>0.969197</td>\n",
       "      <td>0.969543</td>\n",
       "      <td>0.002444</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>614.104580</td>\n",
       "      <td>1.922902</td>\n",
       "      <td>2.830088</td>\n",
       "      <td>0.262840</td>\n",
       "      <td>RandomForestClassifier(bootstrap=False, ccp_al...</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.968110</td>\n",
       "      <td>0.971553</td>\n",
       "      <td>0.968110</td>\n",
       "      <td>0.968835</td>\n",
       "      <td>0.967567</td>\n",
       "      <td>0.972459</td>\n",
       "      <td>0.969197</td>\n",
       "      <td>0.969688</td>\n",
       "      <td>0.001607</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>977.779683</td>\n",
       "      <td>4.340774</td>\n",
       "      <td>4.516003</td>\n",
       "      <td>0.359921</td>\n",
       "      <td>RandomForestClassifier(bootstrap=False, ccp_al...</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>800</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.968835</td>\n",
       "      <td>0.972278</td>\n",
       "      <td>0.967023</td>\n",
       "      <td>0.967204</td>\n",
       "      <td>0.966479</td>\n",
       "      <td>0.972459</td>\n",
       "      <td>0.969922</td>\n",
       "      <td>0.969579</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>972.812523</td>\n",
       "      <td>1.620962</td>\n",
       "      <td>4.564001</td>\n",
       "      <td>0.408978</td>\n",
       "      <td>RandomForestClassifier(bootstrap=False, ccp_al...</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>800</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.969379</td>\n",
       "      <td>0.970466</td>\n",
       "      <td>0.967567</td>\n",
       "      <td>0.968654</td>\n",
       "      <td>0.966298</td>\n",
       "      <td>0.972096</td>\n",
       "      <td>0.969560</td>\n",
       "      <td>0.969434</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>589.785964</td>\n",
       "      <td>0.738173</td>\n",
       "      <td>2.623005</td>\n",
       "      <td>0.096751</td>\n",
       "      <td>RandomForestClassifier(bootstrap=False, ccp_al...</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.967023</td>\n",
       "      <td>0.968110</td>\n",
       "      <td>0.964668</td>\n",
       "      <td>0.964849</td>\n",
       "      <td>0.964486</td>\n",
       "      <td>0.969922</td>\n",
       "      <td>0.965936</td>\n",
       "      <td>0.966879</td>\n",
       "      <td>0.002162</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>588.262862</td>\n",
       "      <td>0.941532</td>\n",
       "      <td>2.698001</td>\n",
       "      <td>0.204831</td>\n",
       "      <td>RandomForestClassifier(bootstrap=False, ccp_al...</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.966842</td>\n",
       "      <td>0.968291</td>\n",
       "      <td>0.965392</td>\n",
       "      <td>0.965755</td>\n",
       "      <td>0.964305</td>\n",
       "      <td>0.971009</td>\n",
       "      <td>0.966298</td>\n",
       "      <td>0.966952</td>\n",
       "      <td>0.001870</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>943.395018</td>\n",
       "      <td>1.991145</td>\n",
       "      <td>4.258004</td>\n",
       "      <td>0.281735</td>\n",
       "      <td>RandomForestClassifier(bootstrap=False, ccp_al...</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>800</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.967567</td>\n",
       "      <td>0.969560</td>\n",
       "      <td>0.965030</td>\n",
       "      <td>0.966117</td>\n",
       "      <td>0.965211</td>\n",
       "      <td>0.969922</td>\n",
       "      <td>0.967385</td>\n",
       "      <td>0.967242</td>\n",
       "      <td>0.001646</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>940.754295</td>\n",
       "      <td>1.497939</td>\n",
       "      <td>4.422000</td>\n",
       "      <td>0.363092</td>\n",
       "      <td>RandomForestClassifier(bootstrap=False, ccp_al...</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>800</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.966661</td>\n",
       "      <td>0.969016</td>\n",
       "      <td>0.963943</td>\n",
       "      <td>0.965030</td>\n",
       "      <td>0.963580</td>\n",
       "      <td>0.970828</td>\n",
       "      <td>0.966298</td>\n",
       "      <td>0.966825</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>610.346955</td>\n",
       "      <td>1.653500</td>\n",
       "      <td>2.731001</td>\n",
       "      <td>0.123647</td>\n",
       "      <td>RandomForestClassifier(bootstrap=False, ccp_al...</td>\n",
       "      <td>False</td>\n",
       "      <td>120</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.969560</td>\n",
       "      <td>0.972096</td>\n",
       "      <td>0.966298</td>\n",
       "      <td>0.968110</td>\n",
       "      <td>0.966479</td>\n",
       "      <td>0.971915</td>\n",
       "      <td>0.971009</td>\n",
       "      <td>0.969633</td>\n",
       "      <td>0.002215</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>609.416506</td>\n",
       "      <td>1.070694</td>\n",
       "      <td>2.845000</td>\n",
       "      <td>0.306700</td>\n",
       "      <td>RandomForestClassifier(bootstrap=False, ccp_al...</td>\n",
       "      <td>False</td>\n",
       "      <td>120</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.969379</td>\n",
       "      <td>0.972096</td>\n",
       "      <td>0.968110</td>\n",
       "      <td>0.968291</td>\n",
       "      <td>0.966842</td>\n",
       "      <td>0.971734</td>\n",
       "      <td>0.969016</td>\n",
       "      <td>0.969923</td>\n",
       "      <td>0.001875</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>974.292969</td>\n",
       "      <td>1.533803</td>\n",
       "      <td>4.750005</td>\n",
       "      <td>0.580119</td>\n",
       "      <td>RandomForestClassifier(bootstrap=False, ccp_al...</td>\n",
       "      <td>False</td>\n",
       "      <td>120</td>\n",
       "      <td>3</td>\n",
       "      <td>800</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.969379</td>\n",
       "      <td>0.971915</td>\n",
       "      <td>0.966842</td>\n",
       "      <td>0.968291</td>\n",
       "      <td>0.966842</td>\n",
       "      <td>0.971915</td>\n",
       "      <td>0.970466</td>\n",
       "      <td>0.969796</td>\n",
       "      <td>0.002199</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>973.742568</td>\n",
       "      <td>1.946723</td>\n",
       "      <td>4.632998</td>\n",
       "      <td>0.353013</td>\n",
       "      <td>RandomForestClassifier(bootstrap=False, ccp_al...</td>\n",
       "      <td>False</td>\n",
       "      <td>120</td>\n",
       "      <td>3</td>\n",
       "      <td>800</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.970103</td>\n",
       "      <td>0.973002</td>\n",
       "      <td>0.966479</td>\n",
       "      <td>0.967748</td>\n",
       "      <td>0.967204</td>\n",
       "      <td>0.972096</td>\n",
       "      <td>0.969922</td>\n",
       "      <td>0.969724</td>\n",
       "      <td>0.002097</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>590.296591</td>\n",
       "      <td>0.581361</td>\n",
       "      <td>2.798001</td>\n",
       "      <td>0.282447</td>\n",
       "      <td>RandomForestClassifier(bootstrap=False, ccp_al...</td>\n",
       "      <td>False</td>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.966661</td>\n",
       "      <td>0.969741</td>\n",
       "      <td>0.964486</td>\n",
       "      <td>0.966842</td>\n",
       "      <td>0.964849</td>\n",
       "      <td>0.969560</td>\n",
       "      <td>0.967204</td>\n",
       "      <td>0.967223</td>\n",
       "      <td>0.001827</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>588.263569</td>\n",
       "      <td>1.037336</td>\n",
       "      <td>2.759002</td>\n",
       "      <td>0.258511</td>\n",
       "      <td>RandomForestClassifier(bootstrap=False, ccp_al...</td>\n",
       "      <td>False</td>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.967204</td>\n",
       "      <td>0.969197</td>\n",
       "      <td>0.964849</td>\n",
       "      <td>0.965936</td>\n",
       "      <td>0.964486</td>\n",
       "      <td>0.969197</td>\n",
       "      <td>0.965392</td>\n",
       "      <td>0.966988</td>\n",
       "      <td>0.001956</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>941.763556</td>\n",
       "      <td>1.751142</td>\n",
       "      <td>4.431001</td>\n",
       "      <td>0.387510</td>\n",
       "      <td>RandomForestClassifier(bootstrap=False, ccp_al...</td>\n",
       "      <td>False</td>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>800</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.966479</td>\n",
       "      <td>0.969741</td>\n",
       "      <td>0.964124</td>\n",
       "      <td>0.965936</td>\n",
       "      <td>0.962856</td>\n",
       "      <td>0.970103</td>\n",
       "      <td>0.967748</td>\n",
       "      <td>0.967060</td>\n",
       "      <td>0.002454</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>940.602730</td>\n",
       "      <td>2.132120</td>\n",
       "      <td>4.420001</td>\n",
       "      <td>0.297249</td>\n",
       "      <td>RandomForestClassifier(bootstrap=False, ccp_al...</td>\n",
       "      <td>False</td>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>800</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.967204</td>\n",
       "      <td>0.969560</td>\n",
       "      <td>0.964668</td>\n",
       "      <td>0.965392</td>\n",
       "      <td>0.964486</td>\n",
       "      <td>0.969197</td>\n",
       "      <td>0.967567</td>\n",
       "      <td>0.967115</td>\n",
       "      <td>0.001903</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>260.760993</td>\n",
       "      <td>1.675265</td>\n",
       "      <td>2.735005</td>\n",
       "      <td>0.120100</td>\n",
       "      <td>RandomForestClassifier(bootstrap=True, ccp_alp...</td>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.965573</td>\n",
       "      <td>0.966479</td>\n",
       "      <td>0.961768</td>\n",
       "      <td>0.963943</td>\n",
       "      <td>0.960681</td>\n",
       "      <td>0.967204</td>\n",
       "      <td>0.966117</td>\n",
       "      <td>0.964959</td>\n",
       "      <td>0.002263</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>423.277381</td>\n",
       "      <td>8.304991</td>\n",
       "      <td>4.292997</td>\n",
       "      <td>0.129000</td>\n",
       "      <td>RandomForestClassifier(bootstrap=True, ccp_alp...</td>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.967748</td>\n",
       "      <td>0.966842</td>\n",
       "      <td>0.963037</td>\n",
       "      <td>0.963399</td>\n",
       "      <td>0.961950</td>\n",
       "      <td>0.967748</td>\n",
       "      <td>0.966298</td>\n",
       "      <td>0.965484</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>254.352069</td>\n",
       "      <td>2.591956</td>\n",
       "      <td>2.592001</td>\n",
       "      <td>0.093996</td>\n",
       "      <td>RandomForestClassifier(bootstrap=True, ccp_alp...</td>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.961587</td>\n",
       "      <td>0.963399</td>\n",
       "      <td>0.958326</td>\n",
       "      <td>0.958507</td>\n",
       "      <td>0.958145</td>\n",
       "      <td>0.963218</td>\n",
       "      <td>0.960500</td>\n",
       "      <td>0.960954</td>\n",
       "      <td>0.002026</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>407.069769</td>\n",
       "      <td>2.604430</td>\n",
       "      <td>4.202998</td>\n",
       "      <td>0.113934</td>\n",
       "      <td>RandomForestClassifier(bootstrap=True, ccp_alp...</td>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.961587</td>\n",
       "      <td>0.963218</td>\n",
       "      <td>0.958326</td>\n",
       "      <td>0.958507</td>\n",
       "      <td>0.958507</td>\n",
       "      <td>0.962493</td>\n",
       "      <td>0.960500</td>\n",
       "      <td>0.961027</td>\n",
       "      <td>0.002023</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>265.334458</td>\n",
       "      <td>3.336179</td>\n",
       "      <td>2.675000</td>\n",
       "      <td>0.066826</td>\n",
       "      <td>RandomForestClassifier(bootstrap=True, ccp_alp...</td>\n",
       "      <td>True</td>\n",
       "      <td>120</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.966117</td>\n",
       "      <td>0.967204</td>\n",
       "      <td>0.962493</td>\n",
       "      <td>0.963399</td>\n",
       "      <td>0.960681</td>\n",
       "      <td>0.966842</td>\n",
       "      <td>0.965392</td>\n",
       "      <td>0.964977</td>\n",
       "      <td>0.002227</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>421.509738</td>\n",
       "      <td>4.777017</td>\n",
       "      <td>4.399005</td>\n",
       "      <td>0.226426</td>\n",
       "      <td>RandomForestClassifier(bootstrap=True, ccp_alp...</td>\n",
       "      <td>True</td>\n",
       "      <td>120</td>\n",
       "      <td>3</td>\n",
       "      <td>800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.966117</td>\n",
       "      <td>0.966479</td>\n",
       "      <td>0.962856</td>\n",
       "      <td>0.963399</td>\n",
       "      <td>0.963580</td>\n",
       "      <td>0.966842</td>\n",
       "      <td>0.966842</td>\n",
       "      <td>0.965285</td>\n",
       "      <td>0.001566</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>255.145714</td>\n",
       "      <td>2.179592</td>\n",
       "      <td>2.621998</td>\n",
       "      <td>0.132048</td>\n",
       "      <td>RandomForestClassifier(bootstrap=True, ccp_alp...</td>\n",
       "      <td>True</td>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.962131</td>\n",
       "      <td>0.962312</td>\n",
       "      <td>0.959051</td>\n",
       "      <td>0.957963</td>\n",
       "      <td>0.956876</td>\n",
       "      <td>0.963037</td>\n",
       "      <td>0.961587</td>\n",
       "      <td>0.960864</td>\n",
       "      <td>0.002146</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>406.991691</td>\n",
       "      <td>2.242739</td>\n",
       "      <td>4.293001</td>\n",
       "      <td>0.185153</td>\n",
       "      <td>RandomForestClassifier(bootstrap=True, ccp_alp...</td>\n",
       "      <td>True</td>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.961950</td>\n",
       "      <td>0.963399</td>\n",
       "      <td>0.957782</td>\n",
       "      <td>0.958869</td>\n",
       "      <td>0.958507</td>\n",
       "      <td>0.964124</td>\n",
       "      <td>0.960500</td>\n",
       "      <td>0.961027</td>\n",
       "      <td>0.002241</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>437.496357</td>\n",
       "      <td>3.462569</td>\n",
       "      <td>2.748464</td>\n",
       "      <td>0.021516</td>\n",
       "      <td>RandomForestClassifier(bootstrap=True, ccp_alp...</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.969560</td>\n",
       "      <td>0.969741</td>\n",
       "      <td>0.966479</td>\n",
       "      <td>0.966117</td>\n",
       "      <td>0.966298</td>\n",
       "      <td>0.971190</td>\n",
       "      <td>0.970103</td>\n",
       "      <td>0.968655</td>\n",
       "      <td>0.001968</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>699.037774</td>\n",
       "      <td>4.686641</td>\n",
       "      <td>4.338999</td>\n",
       "      <td>0.030476</td>\n",
       "      <td>RandomForestClassifier(bootstrap=True, ccp_alp...</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.969379</td>\n",
       "      <td>0.969741</td>\n",
       "      <td>0.966479</td>\n",
       "      <td>0.966842</td>\n",
       "      <td>0.966117</td>\n",
       "      <td>0.970103</td>\n",
       "      <td>0.969741</td>\n",
       "      <td>0.968818</td>\n",
       "      <td>0.001898</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>425.063917</td>\n",
       "      <td>3.340079</td>\n",
       "      <td>2.717340</td>\n",
       "      <td>0.032406</td>\n",
       "      <td>RandomForestClassifier(bootstrap=True, ccp_alp...</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.966479</td>\n",
       "      <td>0.967204</td>\n",
       "      <td>0.962493</td>\n",
       "      <td>0.962131</td>\n",
       "      <td>0.962312</td>\n",
       "      <td>0.967204</td>\n",
       "      <td>0.966117</td>\n",
       "      <td>0.965158</td>\n",
       "      <td>0.002212</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>687.371170</td>\n",
       "      <td>12.391764</td>\n",
       "      <td>4.331006</td>\n",
       "      <td>0.026242</td>\n",
       "      <td>RandomForestClassifier(bootstrap=True, ccp_alp...</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.966661</td>\n",
       "      <td>0.966842</td>\n",
       "      <td>0.963943</td>\n",
       "      <td>0.962131</td>\n",
       "      <td>0.961768</td>\n",
       "      <td>0.967204</td>\n",
       "      <td>0.966117</td>\n",
       "      <td>0.965375</td>\n",
       "      <td>0.002031</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>439.682430</td>\n",
       "      <td>7.218641</td>\n",
       "      <td>2.727005</td>\n",
       "      <td>0.027219</td>\n",
       "      <td>RandomForestClassifier(bootstrap=True, ccp_alp...</td>\n",
       "      <td>False</td>\n",
       "      <td>120</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.969016</td>\n",
       "      <td>0.971009</td>\n",
       "      <td>0.965936</td>\n",
       "      <td>0.967929</td>\n",
       "      <td>0.965030</td>\n",
       "      <td>0.971190</td>\n",
       "      <td>0.969560</td>\n",
       "      <td>0.968927</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>707.656446</td>\n",
       "      <td>14.609961</td>\n",
       "      <td>4.341001</td>\n",
       "      <td>0.094278</td>\n",
       "      <td>RandomForestClassifier(bootstrap=True, ccp_alp...</td>\n",
       "      <td>False</td>\n",
       "      <td>120</td>\n",
       "      <td>3</td>\n",
       "      <td>800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.968110</td>\n",
       "      <td>0.971190</td>\n",
       "      <td>0.966298</td>\n",
       "      <td>0.967023</td>\n",
       "      <td>0.967385</td>\n",
       "      <td>0.970828</td>\n",
       "      <td>0.970103</td>\n",
       "      <td>0.968927</td>\n",
       "      <td>0.001913</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>448.969922</td>\n",
       "      <td>22.223249</td>\n",
       "      <td>2.784113</td>\n",
       "      <td>0.130464</td>\n",
       "      <td>RandomForestClassifier(bootstrap=True, ccp_alp...</td>\n",
       "      <td>False</td>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.965936</td>\n",
       "      <td>0.967748</td>\n",
       "      <td>0.964305</td>\n",
       "      <td>0.963037</td>\n",
       "      <td>0.961950</td>\n",
       "      <td>0.965936</td>\n",
       "      <td>0.965573</td>\n",
       "      <td>0.965448</td>\n",
       "      <td>0.001917</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>754.551990</td>\n",
       "      <td>45.436379</td>\n",
       "      <td>4.708651</td>\n",
       "      <td>0.567820</td>\n",
       "      <td>RandomForestClassifier(bootstrap=True, ccp_alp...</td>\n",
       "      <td>False</td>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.965755</td>\n",
       "      <td>0.966661</td>\n",
       "      <td>0.962312</td>\n",
       "      <td>0.962856</td>\n",
       "      <td>0.962312</td>\n",
       "      <td>0.967023</td>\n",
       "      <td>0.968110</td>\n",
       "      <td>0.965393</td>\n",
       "      <td>0.002076</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      376.445978      3.020293         3.167030        0.361219   \n",
       "1      375.676825      4.016717         2.966644        0.419878   \n",
       "2      613.868614     24.567814         4.498974        0.567725   \n",
       "3      598.148359      3.939247         4.154724        0.210071   \n",
       "4      362.483617      2.267863         2.484566        0.075116   \n",
       "5      362.358065      4.792959         2.441112        0.021331   \n",
       "6      579.947019      7.887904         3.883066        0.031040   \n",
       "7      575.249044      0.880367         3.883574        0.052808   \n",
       "8      376.094352      1.823592         2.520802        0.020453   \n",
       "9      374.463443      1.074614         2.502445        0.021487   \n",
       "10     598.061443      1.165268         4.024759        0.053699   \n",
       "11     596.099226      1.164400         3.983244        0.054331   \n",
       "12     362.536928      1.049495         2.450763        0.022499   \n",
       "13     360.920044      1.841710         2.435615        0.026228   \n",
       "14     576.560057      0.780648         3.857213        0.033624   \n",
       "15     575.936539      1.421310         3.862327        0.033132   \n",
       "16     616.162348      1.117404         2.670657        0.022961   \n",
       "17     614.104580      1.922902         2.830088        0.262840   \n",
       "18     977.779683      4.340774         4.516003        0.359921   \n",
       "19     972.812523      1.620962         4.564001        0.408978   \n",
       "20     589.785964      0.738173         2.623005        0.096751   \n",
       "21     588.262862      0.941532         2.698001        0.204831   \n",
       "22     943.395018      1.991145         4.258004        0.281735   \n",
       "23     940.754295      1.497939         4.422000        0.363092   \n",
       "24     610.346955      1.653500         2.731001        0.123647   \n",
       "25     609.416506      1.070694         2.845000        0.306700   \n",
       "26     974.292969      1.533803         4.750005        0.580119   \n",
       "27     973.742568      1.946723         4.632998        0.353013   \n",
       "28     590.296591      0.581361         2.798001        0.282447   \n",
       "29     588.263569      1.037336         2.759002        0.258511   \n",
       "30     941.763556      1.751142         4.431001        0.387510   \n",
       "31     940.602730      2.132120         4.420001        0.297249   \n",
       "32     260.760993      1.675265         2.735005        0.120100   \n",
       "33     423.277381      8.304991         4.292997        0.129000   \n",
       "34     254.352069      2.591956         2.592001        0.093996   \n",
       "35     407.069769      2.604430         4.202998        0.113934   \n",
       "36     265.334458      3.336179         2.675000        0.066826   \n",
       "37     421.509738      4.777017         4.399005        0.226426   \n",
       "38     255.145714      2.179592         2.621998        0.132048   \n",
       "39     406.991691      2.242739         4.293001        0.185153   \n",
       "40     437.496357      3.462569         2.748464        0.021516   \n",
       "41     699.037774      4.686641         4.338999        0.030476   \n",
       "42     425.063917      3.340079         2.717340        0.032406   \n",
       "43     687.371170     12.391764         4.331006        0.026242   \n",
       "44     439.682430      7.218641         2.727005        0.027219   \n",
       "45     707.656446     14.609961         4.341001        0.094278   \n",
       "46     448.969922     22.223249         2.784113        0.130464   \n",
       "47     754.551990     45.436379         4.708651        0.567820   \n",
       "\n",
       "                                             param_dt param_dt__bootstrap  \\\n",
       "0   RandomForestClassifier(bootstrap=False, ccp_al...                True   \n",
       "1   RandomForestClassifier(bootstrap=False, ccp_al...                True   \n",
       "2   RandomForestClassifier(bootstrap=False, ccp_al...                True   \n",
       "3   RandomForestClassifier(bootstrap=False, ccp_al...                True   \n",
       "4   RandomForestClassifier(bootstrap=False, ccp_al...                True   \n",
       "5   RandomForestClassifier(bootstrap=False, ccp_al...                True   \n",
       "6   RandomForestClassifier(bootstrap=False, ccp_al...                True   \n",
       "7   RandomForestClassifier(bootstrap=False, ccp_al...                True   \n",
       "8   RandomForestClassifier(bootstrap=False, ccp_al...                True   \n",
       "9   RandomForestClassifier(bootstrap=False, ccp_al...                True   \n",
       "10  RandomForestClassifier(bootstrap=False, ccp_al...                True   \n",
       "11  RandomForestClassifier(bootstrap=False, ccp_al...                True   \n",
       "12  RandomForestClassifier(bootstrap=False, ccp_al...                True   \n",
       "13  RandomForestClassifier(bootstrap=False, ccp_al...                True   \n",
       "14  RandomForestClassifier(bootstrap=False, ccp_al...                True   \n",
       "15  RandomForestClassifier(bootstrap=False, ccp_al...                True   \n",
       "16  RandomForestClassifier(bootstrap=False, ccp_al...               False   \n",
       "17  RandomForestClassifier(bootstrap=False, ccp_al...               False   \n",
       "18  RandomForestClassifier(bootstrap=False, ccp_al...               False   \n",
       "19  RandomForestClassifier(bootstrap=False, ccp_al...               False   \n",
       "20  RandomForestClassifier(bootstrap=False, ccp_al...               False   \n",
       "21  RandomForestClassifier(bootstrap=False, ccp_al...               False   \n",
       "22  RandomForestClassifier(bootstrap=False, ccp_al...               False   \n",
       "23  RandomForestClassifier(bootstrap=False, ccp_al...               False   \n",
       "24  RandomForestClassifier(bootstrap=False, ccp_al...               False   \n",
       "25  RandomForestClassifier(bootstrap=False, ccp_al...               False   \n",
       "26  RandomForestClassifier(bootstrap=False, ccp_al...               False   \n",
       "27  RandomForestClassifier(bootstrap=False, ccp_al...               False   \n",
       "28  RandomForestClassifier(bootstrap=False, ccp_al...               False   \n",
       "29  RandomForestClassifier(bootstrap=False, ccp_al...               False   \n",
       "30  RandomForestClassifier(bootstrap=False, ccp_al...               False   \n",
       "31  RandomForestClassifier(bootstrap=False, ccp_al...               False   \n",
       "32  RandomForestClassifier(bootstrap=True, ccp_alp...                True   \n",
       "33  RandomForestClassifier(bootstrap=True, ccp_alp...                True   \n",
       "34  RandomForestClassifier(bootstrap=True, ccp_alp...                True   \n",
       "35  RandomForestClassifier(bootstrap=True, ccp_alp...                True   \n",
       "36  RandomForestClassifier(bootstrap=True, ccp_alp...                True   \n",
       "37  RandomForestClassifier(bootstrap=True, ccp_alp...                True   \n",
       "38  RandomForestClassifier(bootstrap=True, ccp_alp...                True   \n",
       "39  RandomForestClassifier(bootstrap=True, ccp_alp...                True   \n",
       "40  RandomForestClassifier(bootstrap=True, ccp_alp...               False   \n",
       "41  RandomForestClassifier(bootstrap=True, ccp_alp...               False   \n",
       "42  RandomForestClassifier(bootstrap=True, ccp_alp...               False   \n",
       "43  RandomForestClassifier(bootstrap=True, ccp_alp...               False   \n",
       "44  RandomForestClassifier(bootstrap=True, ccp_alp...               False   \n",
       "45  RandomForestClassifier(bootstrap=True, ccp_alp...               False   \n",
       "46  RandomForestClassifier(bootstrap=True, ccp_alp...               False   \n",
       "47  RandomForestClassifier(bootstrap=True, ccp_alp...               False   \n",
       "\n",
       "   param_dt__max_depth param_dt__min_samples_leaf param_dt__n_estimators  \\\n",
       "0                  100                          3                    500   \n",
       "1                  100                          3                    500   \n",
       "2                  100                          3                    800   \n",
       "3                  100                          3                    800   \n",
       "4                  100                          5                    500   \n",
       "5                  100                          5                    500   \n",
       "6                  100                          5                    800   \n",
       "7                  100                          5                    800   \n",
       "8                  120                          3                    500   \n",
       "9                  120                          3                    500   \n",
       "10                 120                          3                    800   \n",
       "11                 120                          3                    800   \n",
       "12                 120                          5                    500   \n",
       "13                 120                          5                    500   \n",
       "14                 120                          5                    800   \n",
       "15                 120                          5                    800   \n",
       "16                 100                          3                    500   \n",
       "17                 100                          3                    500   \n",
       "18                 100                          3                    800   \n",
       "19                 100                          3                    800   \n",
       "20                 100                          5                    500   \n",
       "21                 100                          5                    500   \n",
       "22                 100                          5                    800   \n",
       "23                 100                          5                    800   \n",
       "24                 120                          3                    500   \n",
       "25                 120                          3                    500   \n",
       "26                 120                          3                    800   \n",
       "27                 120                          3                    800   \n",
       "28                 120                          5                    500   \n",
       "29                 120                          5                    500   \n",
       "30                 120                          5                    800   \n",
       "31                 120                          5                    800   \n",
       "32                 100                          3                    500   \n",
       "33                 100                          3                    800   \n",
       "34                 100                          5                    500   \n",
       "35                 100                          5                    800   \n",
       "36                 120                          3                    500   \n",
       "37                 120                          3                    800   \n",
       "38                 120                          5                    500   \n",
       "39                 120                          5                    800   \n",
       "40                 100                          3                    500   \n",
       "41                 100                          3                    800   \n",
       "42                 100                          5                    500   \n",
       "43                 100                          5                    800   \n",
       "44                 120                          3                    500   \n",
       "45                 120                          3                    800   \n",
       "46                 120                          5                    500   \n",
       "47                 120                          5                    800   \n",
       "\n",
       "   param_scaler__with_std  ... split3_test_score split4_test_score  \\\n",
       "0                    True  ...          0.965755          0.968654   \n",
       "1                   False  ...          0.965030          0.969560   \n",
       "2                    True  ...          0.965392          0.967929   \n",
       "3                   False  ...          0.965573          0.968110   \n",
       "4                    True  ...          0.963218          0.965030   \n",
       "5                   False  ...          0.963399          0.965573   \n",
       "6                    True  ...          0.962312          0.964668   \n",
       "7                   False  ...          0.963762          0.965030   \n",
       "8                    True  ...          0.965030          0.968835   \n",
       "9                   False  ...          0.966298          0.968654   \n",
       "10                   True  ...          0.966661          0.968473   \n",
       "11                  False  ...          0.965755          0.968473   \n",
       "12                   True  ...          0.962493          0.965211   \n",
       "13                  False  ...          0.964124          0.966117   \n",
       "14                   True  ...          0.963037          0.965211   \n",
       "15                  False  ...          0.962131          0.965211   \n",
       "16                   True  ...          0.969741          0.973365   \n",
       "17                  False  ...          0.968110          0.971553   \n",
       "18                   True  ...          0.968835          0.972278   \n",
       "19                  False  ...          0.969379          0.970466   \n",
       "20                   True  ...          0.967023          0.968110   \n",
       "21                  False  ...          0.966842          0.968291   \n",
       "22                   True  ...          0.967567          0.969560   \n",
       "23                  False  ...          0.966661          0.969016   \n",
       "24                   True  ...          0.969560          0.972096   \n",
       "25                  False  ...          0.969379          0.972096   \n",
       "26                   True  ...          0.969379          0.971915   \n",
       "27                  False  ...          0.970103          0.973002   \n",
       "28                   True  ...          0.966661          0.969741   \n",
       "29                  False  ...          0.967204          0.969197   \n",
       "30                   True  ...          0.966479          0.969741   \n",
       "31                  False  ...          0.967204          0.969560   \n",
       "32                    NaN  ...          0.965573          0.966479   \n",
       "33                    NaN  ...          0.967748          0.966842   \n",
       "34                    NaN  ...          0.961587          0.963399   \n",
       "35                    NaN  ...          0.961587          0.963218   \n",
       "36                    NaN  ...          0.966117          0.967204   \n",
       "37                    NaN  ...          0.966117          0.966479   \n",
       "38                    NaN  ...          0.962131          0.962312   \n",
       "39                    NaN  ...          0.961950          0.963399   \n",
       "40                    NaN  ...          0.969560          0.969741   \n",
       "41                    NaN  ...          0.969379          0.969741   \n",
       "42                    NaN  ...          0.966479          0.967204   \n",
       "43                    NaN  ...          0.966661          0.966842   \n",
       "44                    NaN  ...          0.969016          0.971009   \n",
       "45                    NaN  ...          0.968110          0.971190   \n",
       "46                    NaN  ...          0.965936          0.967748   \n",
       "47                    NaN  ...          0.965755          0.966661   \n",
       "\n",
       "   split5_test_score  split6_test_score  split7_test_score  split8_test_score  \\\n",
       "0           0.962131           0.963580           0.963762           0.968110   \n",
       "1           0.964305           0.963399           0.963580           0.969379   \n",
       "2           0.964124           0.964486           0.963218           0.967929   \n",
       "3           0.964849           0.964668           0.962493           0.968110   \n",
       "4           0.960138           0.962131           0.959051           0.964305   \n",
       "5           0.960138           0.960500           0.960681           0.965211   \n",
       "6           0.961406           0.961768           0.961225           0.966117   \n",
       "7           0.960500           0.961587           0.960500           0.965573   \n",
       "8           0.963580           0.963580           0.963943           0.967567   \n",
       "9           0.961950           0.963580           0.962312           0.967748   \n",
       "10          0.963037           0.963943           0.963580           0.968654   \n",
       "11          0.964305           0.965030           0.962493           0.968835   \n",
       "12          0.960681           0.961044           0.961225           0.965211   \n",
       "13          0.960500           0.960681           0.959594           0.965936   \n",
       "14          0.960862           0.960862           0.960681           0.965030   \n",
       "15          0.960862           0.960862           0.961044           0.966298   \n",
       "16          0.966842           0.966479           0.966842           0.971553   \n",
       "17          0.968110           0.968835           0.967567           0.972459   \n",
       "18          0.967023           0.967204           0.966479           0.972459   \n",
       "19          0.967567           0.968654           0.966298           0.972096   \n",
       "20          0.964668           0.964849           0.964486           0.969922   \n",
       "21          0.965392           0.965755           0.964305           0.971009   \n",
       "22          0.965030           0.966117           0.965211           0.969922   \n",
       "23          0.963943           0.965030           0.963580           0.970828   \n",
       "24          0.966298           0.968110           0.966479           0.971915   \n",
       "25          0.968110           0.968291           0.966842           0.971734   \n",
       "26          0.966842           0.968291           0.966842           0.971915   \n",
       "27          0.966479           0.967748           0.967204           0.972096   \n",
       "28          0.964486           0.966842           0.964849           0.969560   \n",
       "29          0.964849           0.965936           0.964486           0.969197   \n",
       "30          0.964124           0.965936           0.962856           0.970103   \n",
       "31          0.964668           0.965392           0.964486           0.969197   \n",
       "32          0.961768           0.963943           0.960681           0.967204   \n",
       "33          0.963037           0.963399           0.961950           0.967748   \n",
       "34          0.958326           0.958507           0.958145           0.963218   \n",
       "35          0.958326           0.958507           0.958507           0.962493   \n",
       "36          0.962493           0.963399           0.960681           0.966842   \n",
       "37          0.962856           0.963399           0.963580           0.966842   \n",
       "38          0.959051           0.957963           0.956876           0.963037   \n",
       "39          0.957782           0.958869           0.958507           0.964124   \n",
       "40          0.966479           0.966117           0.966298           0.971190   \n",
       "41          0.966479           0.966842           0.966117           0.970103   \n",
       "42          0.962493           0.962131           0.962312           0.967204   \n",
       "43          0.963943           0.962131           0.961768           0.967204   \n",
       "44          0.965936           0.967929           0.965030           0.971190   \n",
       "45          0.966298           0.967023           0.967385           0.970828   \n",
       "46          0.964305           0.963037           0.961950           0.965936   \n",
       "47          0.962312           0.962856           0.962312           0.967023   \n",
       "\n",
       "    split9_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0            0.964305         0.965810        0.002267               24  \n",
       "1            0.965392         0.965774        0.002256               25  \n",
       "2            0.966479         0.965919        0.001937               22  \n",
       "3            0.965211         0.965629        0.001715               27  \n",
       "4            0.960862         0.962350        0.002067               44  \n",
       "5            0.961768         0.962929        0.002121               38  \n",
       "6            0.961406         0.962730        0.001603               42  \n",
       "7            0.962312         0.963020        0.001828               37  \n",
       "8            0.965936         0.965720        0.001865               26  \n",
       "9            0.965936         0.965629        0.002392               28  \n",
       "10           0.964124         0.965810        0.001974               23  \n",
       "11           0.966117         0.965991        0.001895               21  \n",
       "12           0.962131         0.962766        0.001610               41  \n",
       "13           0.960138         0.962893        0.002528               39  \n",
       "14           0.962312         0.962784        0.001689               40  \n",
       "15           0.962131         0.962730        0.001942               43  \n",
       "16           0.969197         0.969543        0.002444                7  \n",
       "17           0.969197         0.969688        0.001607                4  \n",
       "18           0.969922         0.969579        0.002193                6  \n",
       "19           0.969560         0.969434        0.001753                8  \n",
       "20           0.965936         0.966879        0.002162               19  \n",
       "21           0.966298         0.966952        0.001870               18  \n",
       "22           0.967385         0.967242        0.001646               13  \n",
       "23           0.966298         0.966825        0.002248               20  \n",
       "24           0.971009         0.969633        0.002215                5  \n",
       "25           0.969016         0.969923        0.001875                1  \n",
       "26           0.970466         0.969796        0.002199                2  \n",
       "27           0.969922         0.969724        0.002097                3  \n",
       "28           0.967204         0.967223        0.001827               14  \n",
       "29           0.965392         0.966988        0.001956               17  \n",
       "30           0.967748         0.967060        0.002454               16  \n",
       "31           0.967567         0.967115        0.001903               15  \n",
       "32           0.966117         0.964959        0.002263               36  \n",
       "33           0.966298         0.965484        0.002083               29  \n",
       "34           0.960500         0.960954        0.002026               47  \n",
       "35           0.960500         0.961027        0.002023               46  \n",
       "36           0.965392         0.964977        0.002227               35  \n",
       "37           0.966842         0.965285        0.001566               33  \n",
       "38           0.961587         0.960864        0.002146               48  \n",
       "39           0.960500         0.961027        0.002241               45  \n",
       "40           0.970103         0.968655        0.001968               12  \n",
       "41           0.969741         0.968818        0.001898               11  \n",
       "42           0.966117         0.965158        0.002212               34  \n",
       "43           0.966117         0.965375        0.002031               32  \n",
       "44           0.969560         0.968927        0.002048               10  \n",
       "45           0.970103         0.968927        0.001913                9  \n",
       "46           0.965573         0.965448        0.001917               30  \n",
       "47           0.968110         0.965393        0.002076               31  \n",
       "\n",
       "[48 rows x 26 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(rf_model.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_scaler</th>\n",
       "      <th>param_scaler__threshold</th>\n",
       "      <th>param_xgbrg</th>\n",
       "      <th>param_xgbrg__max_depth</th>\n",
       "      <th>param_xgbrg__num_class</th>\n",
       "      <th>param_xgbrg__objective</th>\n",
       "      <th>...</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split8_test_score</th>\n",
       "      <th>split9_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3976.697375</td>\n",
       "      <td>28.691386</td>\n",
       "      <td>2.196011</td>\n",
       "      <td>0.105567</td>\n",
       "      <td>Binarizer(copy=True, threshold=18.257908978953...</td>\n",
       "      <td>18.2579</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>multi:softmax</td>\n",
       "      <td>...</td>\n",
       "      <td>0.970103</td>\n",
       "      <td>0.976083</td>\n",
       "      <td>0.971009</td>\n",
       "      <td>0.973727</td>\n",
       "      <td>0.968110</td>\n",
       "      <td>0.974633</td>\n",
       "      <td>0.973727</td>\n",
       "      <td>0.972007</td>\n",
       "      <td>0.002731</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4456.115117</td>\n",
       "      <td>28.953306</td>\n",
       "      <td>2.088107</td>\n",
       "      <td>0.243607</td>\n",
       "      <td>StandardScaler(copy=True, with_mean=True, with...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>multi:softmax</td>\n",
       "      <td>...</td>\n",
       "      <td>0.968835</td>\n",
       "      <td>0.973546</td>\n",
       "      <td>0.967748</td>\n",
       "      <td>0.970103</td>\n",
       "      <td>0.966842</td>\n",
       "      <td>0.973727</td>\n",
       "      <td>0.969741</td>\n",
       "      <td>0.970068</td>\n",
       "      <td>0.002331</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0    3976.697375     28.691386         2.196011        0.105567   \n",
       "1    4456.115117     28.953306         2.088107        0.243607   \n",
       "\n",
       "                                        param_scaler param_scaler__threshold  \\\n",
       "0  Binarizer(copy=True, threshold=18.257908978953...                 18.2579   \n",
       "1  StandardScaler(copy=True, with_mean=True, with...                     NaN   \n",
       "\n",
       "                                         param_xgbrg param_xgbrg__max_depth  \\\n",
       "0  XGBClassifier(base_score=0.5, booster='gbtree'...                     10   \n",
       "1  XGBClassifier(base_score=0.5, booster='gbtree'...                     10   \n",
       "\n",
       "  param_xgbrg__num_class param_xgbrg__objective  ... split3_test_score  \\\n",
       "0                     10          multi:softmax  ...          0.970103   \n",
       "1                     10          multi:softmax  ...          0.968835   \n",
       "\n",
       "   split4_test_score  split5_test_score  split6_test_score  split7_test_score  \\\n",
       "0           0.976083           0.971009           0.973727           0.968110   \n",
       "1           0.973546           0.967748           0.970103           0.966842   \n",
       "\n",
       "   split8_test_score  split9_test_score  mean_test_score  std_test_score  \\\n",
       "0           0.974633           0.973727         0.972007        0.002731   \n",
       "1           0.973727           0.969741         0.970068        0.002331   \n",
       "\n",
       "   rank_test_score  \n",
       "0                1  \n",
       "1                2  \n",
       "\n",
       "[2 rows x 24 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(xgrb_model.cv_results_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
